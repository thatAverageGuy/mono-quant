---
phase: 01-core-quantization-foundation
plan: 04
type: execute
wave: 3
depends_on: [01-03]
files_modified:
  - src/mono_quant/__init__.py
  - src/mono_quant/core/quantizers.py
autonomous: false

must_haves:
  truths:
    - "User can call dynamic_quantize(model, dtype=torch.qint8) to quantize a model"
    - "User can call dynamic_quantize(model, dtype=torch.float16) for FP16"
    - "Quantized model returns different object (original not modified)"
    - "Function returns list of skipped layer names"
    - "Public API exports are available from mono_quant package"
  artifacts:
    - path: "src/mono_quant/__init__.py"
      provides: "Public API exports"
      exports: ["QuantizationConfig", "dynamic_quantize"]
    - path: "src/mono_quant/core/quantizers.py"
      provides: "End-to-end dynamic quantization"
      exports: ["dynamic_quantize"]
  key_links:
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/io/handlers.py"
      via: "import"
      pattern: "from mono_quant.io.handlers import"
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/modules/linear.py"
      via: "import"
      pattern: "from mono_quant.modules.linear import"
---

<objective>
Create the end-to-end dynamic quantization flow and expose the public API. This plan wires together all the components from previous plans into a user-facing quantize() function that accepts models and returns quantized versions.

Purpose: Provide a simple, working API for users to quantize PyTorch models
Output: Working dynamic_quantize() function with public API exports
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-quantization-foundation/01-CONTEXT.md
@.planning/phases/01-core-quantization-foundation/01-RESEARCH.md
@.planning/REQUIREMENTS.md

RESEARCH.md patterns to follow:
- Code Examples: Dynamic INT8 Quantization (full model flow)
- Pattern 1: Model-Agnostic Input Handling
- Pattern 3: FP16 Quantization

CONTEXT.md decisions:
- Always copy the model
- Unsupported layers: Partial quantization (return skipped list)
- Configuration priority: function parameters > config > defaults
- Flexible return (both nn.Module and state_dict, or user choice)

Requirements covered:
- AGN-01: System accepts any PyTorch nn.Module
- QCORE-01: Quantize to INT8 with per-channel scaling
- QCORE-03: Quantize to FP16
- QCORE-04: Dynamic quantization (no calibration data)
- QCORE-07: Choose symmetric or asymmetric scheme
</context>

<tasks>

<task type="auto">
  <name>Create dynamic_quantize function</name>
  <files>src/mono_quant/core/quantizers.py</files>
  <action>
Add to src/mono_quant/core/quantizers.py:

1. Import _prepare_model from mono_quant.io.handlers
2. Import quantize_linear_module from mono_quant.modules.linear

2. Create dynamic_quantize() function:
   - Parameters:
     - model: Union[nn.Module, Dict[str, torch.Tensor]]
     - dtype: torch.dtype = torch.qint8
     - symmetric: bool = False
     - per_channel: bool = True
     - config: Optional[QuantizationConfig] = None
   - Returns: Tuple[nn.Module, List[str]] (quantized model, skipped layers)

3. Implementation:
   - Resolve config: if config provided, use its values (dtype, symmetric, per_channel)
   - Call _prepare_model(model) to get a copy
   - Initialize skipped = []
   - Iterate over named_children():
     - If isinstance(module, nn.Linear):
       - Replace with quantize_linear_module(module, dtype=dtype, symmetric=symmetric)
     - Else if isinstance(module, nn.Conv2d):
       - Apply per-channel quantization to weight
       - Create new Conv2d with quantized weight
     - Else:
       - Add to skipped list (partial quantization per CONTEXT.md)
   - Return (model_copy, skipped)

4. Create _quantize_fp16_model() helper:
   - Simpler flow: iterate all parameters and call .to(torch.float16)
   - No layer type filtering needed
   - Return (model_copy, []) (no skipped layers for FP16)

5. Update dynamic_quantize() to dispatch:
   - If dtype == torch.float16: call _quantize_fp16_model()
   - Else: use layer-specific quantization

Export dynamic_quantize from src/mono_quant/core/__init__.py
  </action>
  <verify>python -c "from mono_quant.core import dynamic_quantize; import torch.nn as nn; m = nn.Linear(10, 10); qm, skipped = dynamic_quantize(m); print('Quantized, skipped:', len(skipped))"</verify>
  <done>dynamic_quantize function quantizes models to INT8 or FP16</done>
</task>

<task type="auto">
  <name>Expose public API from package root</name>
  <files>src/mono_quant/__init__.py</files>
  <action>
Update src/mono_quant/__init__.py to expose public API:

1. Add package metadata:
   - __version__ = "0.1.0"
   - __all__ list with exports

2. Import and re-export:
   - QuantizationConfig from mono_quant.config
   - dynamic_quantize from mono_quant.core

3. Add docstring at package level explaining:
   - "Mono Quant - Simple, reliable model quantization with minimal dependencies"
   - Quick example: dynamic_quantize(model, dtype=torch.qint8)

DO NOT export internal functions (prefixed with _)
DO NOT export implementation details (schemes, mappers, handlers unless explicitly requested)
  </action>
  <verify>python -c "from mono_quant import dynamic_quantize, QuantizationConfig; print('API exports work')"</verify>
  <done>Public API is accessible from mono_quant package root</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete dynamic quantization flow with public API</what-built>
  <how-to-verify>
1. Create a test model:
   ```python
   import torch
   import torch.nn as nn
   from mono_quant import dynamic_quantize

   model = nn.Sequential(
       nn.Linear(128, 256),
       nn.ReLU(),
       nn.Linear(256, 10)
   )

   # Test INT8 quantization
   q_model_int8, skipped = dynamic_quantize(model, dtype=torch.qint8)
   print(f"INT8: Skipped {len(skipped)} layers: {skipped}")

   # Test FP16 quantization
   q_model_fp16, skipped = dynamic_quantize(model, dtype=torch.float16)
   print(f"FP16: Skipped {len(skipped)} layers: {skipped}")

   # Verify original unchanged
   original_output = model(torch.randn(1, 128))
   quantized_output = q_model_int8(torch.randn(1, 128))
   print(f"Outputs have shape: {original_output.shape}, {quantized_output.shape}")

   # Check memory reduction
   import torch
   print(f"Original size: {sum(p.numel() * p.element_size() for p in model.parameters())} bytes")
   print(f"Quantized size: {sum(p.numel() * p.element_size() for p in q_model_int8.parameters())} bytes")
   ```

2. Run in Python: The code should execute without errors
3. Verify: skipped list contains non-quantizable layers (ReLU)
4. Verify: Original model id is different from quantized model id
5. Verify: Model size is reduced after quantization
  </how-to-verify>
  <resume-signal>Type "approved" if quantization works correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
Run end-to-end test:
1. Import from mono_quant (not submodules) works
2. dynamic_quantize returns quantized model
3. Original model is unchanged (id check)
4. Skipped layers list is populated correctly
5. Quantized model can run inference
</verification>

<success_criteria>
- User can call from mono_quant import dynamic_quantize
- dynamic_quantize(model) returns quantized model and skipped list
- Original model is unchanged (verifiable by id check)
- INT8 quantization reduces model size
- FP16 quantization reduces model size
- Both quantized models can run inference
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-quantization-foundation/01-04-SUMMARY.md`
</output>
