---
phase: 01-core-quantization-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/mono_quant/core/schemes.py
  - src/mono_quant/core/mappers.py
autonomous: true

must_haves:
  truths:
    - "Symmetric quantization scheme calculates scale from max absolute value"
    - "Asymmetric quantization scheme calculates scale and zero-point from min/max"
    - "Per-channel scale calculation reduces over all dimensions except axis 0"
    - "Scale calculation handles edge case of zero range (clamps to min value)"
  artifacts:
    - path: "src/mono_quant/core/schemes.py"
      provides: "Quantization scheme classes"
      exports: ["SymmetricScheme", "AsymmetricScheme"]
    - path: "src/mono_quant/core/mappers.py"
      provides: "Scale and zero-point calculation functions"
      exports: ["calculate_scale_zp_per_tensor", "calculate_scale_zp_per_channel"]
  key_links:
    - from: "src/mono_quant/core/schemes.py"
      to: "src/mono_quant/core/mappers.py"
      via: "import"
      pattern: "from mono_quant.core.mappers import"
---

<objective>
Implement core quantization math: symmetric and asymmetric quantization schemes with per-tensor and per-channel scale/zero-point calculation. These are pure mathematical functions with no PyTorch module dependencies, making them highly testable.

Purpose: Provide the foundational quantization mathematics used by all quantization operations
Output: Testable quantization schemes and scale/zero-point mappers
</objective>

<execution_context>
@C:\Users\ghost\.thatAverageGuy\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.thatAverageGuy\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-quantization-foundation/01-CONTEXT.md
@.planning/phases/01-core-quantization-foundation/01-RESEARCH.md
@.planning/REQUIREMENTS.md

RESEARCH.md patterns to follow:
- Pattern 2: Dynamic Quantization with Per-Channel Scaling (for scale/zp formulas)
- Symmetric vs Asymmetric formulas from NVIDIA blog
- qmin/qmax ranges for int8: -128 to 127

Key CONTEXT.md decisions:
- Symmetric vs asymmetric is layer-dependent (provide both, let caller choose)
- thatAverageGuy's discretion on internal API granularity
</context>

<tasks>

<task type="auto">
  <name>Create quantization scheme classes</name>
  <files>src/mono_quant/core/schemes.py</files>
  <action>
Create src/mono_quant/core/schemes.py with:

1. Import torch, dataclass, Optional

2. Create abstract base class QuantizationScheme:
   - Abstract method: calculate(tensor: torch.Tensor, axis: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]
   - Docstring explaining return is (scale, zero_point)

3. Create SymmetricScheme class (inherits QuantizationScheme):
   - Constructor parameters: qmin: int = -128, qmax: int = 127
   - calculate() method:
     - If axis is None (per-tensor): use tensor.abs().max()
     - If axis is int (per-channel): use tensor.abs().amax(dim=axis)
     - scale = max_abs / qmax
     - zero_point = torch.zeros_like(scale) (symmetric has zero offset)
     - Clamp scale to min=1e-8 to avoid division by zero
     - Return (scale, zero_point)

4. Create AsymmetricScheme class (inherits QuantizationScheme):
   - Constructor parameters: qmin: int = -128, qmax: int = 127
   - calculate() method:
     - If axis is None: use tensor.amin(), tensor.amax()
     - If axis is int: use tensor.amin(dim=axis), tensor.amax(dim=axis)
     - scale = (max_val - min_val) / (qmax - qmin)
     - zero_point = qmin - (min_val / scale)
     - Clamp scale to min=1e-8
     - Return (scale, zero_point.int())

Export classes from src/mono_quant/core/__init__.py
  </action>
  <verify>python -c "from mono_quant.core import SymmetricScheme; import torch; s = SymmetricScheme(); scale, zp = s.calculate(torch.randn(10, 10)); print(scale.shape, zp.shape)"</verify>
  <done>Scheme classes calculate scale and zero-point for symmetric and asymmetric quantization</done>
</task>

<task type="auto">
  <name>Create scale and zero-point mapper functions</name>
  <files>src/mono_quant/core/mappers.py</files>
  <action>
Create src/mono_quant/core/mappers.py with:

1. Import torch, Tuple, Optional

2. Create calculate_scale_zp_per_tensor() function:
   - Parameters: tensor, dtype=torch.qint8, symmetric=False
   - Set qmin, qmax based on dtype:
     - torch.qint8: -128, 127
     - torch.float16: return (None, None) - FP16 doesn't use scale/zp
   - If symmetric: scale = tensor.abs().max() / qmax, zp = 0
   - If asymmetric: scale = (max - min) / (qmax - qmin), zp = qmin - (min / scale)
   - Clamp scale to min=1e-8
   - Return (scale, zp)

3. Create calculate_scale_zp_per_channel() function:
   - Parameters: tensor, dtype=torch.qint8, symmetric=False, axis=0
   - Same logic as per_tensor, but reduce over specified axis
   - Use tuple(range(1, tensor.dim())) if axis=0 to reduce over all except first dim
   - For axis=0 specifically (standard for per-channel weights):
     - Use tensor.amin(dim=axis, keepdim=True) for correct broadcasting
   - Return (scale, zp) with shapes matching tensor.shape[axis]

4. Create get_dtype_range() helper:
   - Input: torch.dtype
   - Returns: Tuple[int, int] (qmin, qmax)
   - Handle torch.qint8, torch.quint8, torch.qint4
   - Raise ValueError for unsupported dtypes

Export functions from src/mono_quant/core/__init__.py
  </action>
  <verify>python -c "from mono_quant.core.mappers import calculate_scale_zp_per_channel; import torch; w = torch.randn(64, 128); s, z = calculate_scale_zp_per_channel(w); assert s.shape == (64,)</verify>
  <done>Mapper functions calculate per-tensor and per-channel scale/zero-point</done>
</task>

</tasks>

<verification>
Create test verification:
1. Symmetric scheme produces zero_point = 0
2. Asymmetric scheme produces non-zero zero_point for asymmetric data
3. Per-channel calculation produces correct shape (scale.shape[axis] == tensor.shape[axis])
4. Scale is clamped to avoid division by zero
</verification>

<success_criteria>
- SymmetricScheme calculates scale from max abs, zero_point = 0
- AsymmetricScheme calculates scale from range, zero_point from offset
- Per-channel reduces over correct axis
- Functions handle edge cases (zero range, single value)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-quantization-foundation/01-02-SUMMARY.md`
</output>
