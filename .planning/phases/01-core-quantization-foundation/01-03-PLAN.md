---
phase: 01-core-quantization-foundation
plan: 03
type: execute
wave: 2
depends_on: [01-01, 01-02]
files_modified:
  - src/mono_quant/core/quantizers.py
  - src/mono_quant/modules/__init__.py
  - src/mono_quant/modules/linear.py
autonomous: true

must_haves:
  truths:
    - "INT8 weights can be quantized using torch.quantize_per_channel"
    - "FP16 weights can be converted using simple dtype casting"
    - "QuantizedLinear module wraps quantized weights for inference"
    - "Conv2d modules can be quantized with per-channel scaling (no placeholder)"
    - "Bias is preserved when quantizing Linear and Conv2d layers"
  artifacts:
    - path: "src/mono_quant/core/quantizers.py"
      provides: "Quantization transformation functions"
      exports: ["quantize_weight_int8", "quantize_weight_fp16"]
    - path: "src/mono_quant/modules/linear.py"
      provides: "QuantizedLinear and quantization helper functions"
      exports: ["QuantizedLinear", "quantize_linear_module", "quantize_conv2d_module"]
  key_links:
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/core/mappers.py"
      via: "import"
      pattern: "from mono_quant.core.mappers import"
    - from: "src/mono_quant/core/quantizers.py"
      to: "torch.quantize_per_channel"
      via: "torch"
      pattern: "torch.quantize_per_channel"
---

<objective>
Implement quantization transformations for INT8 and FP16 weights, plus a QuantizedLinear module. This plan creates the actual quantization operations that transform floating-point weights to quantized representations.

Purpose: Apply quantization to model weights using PyTorch's native quantization primitives
Output: Working quantization functions for INT8 (per-channel) and FP16 (dtype cast)
</objective>

<execution_context>
@C:\Users\ghost\.thatAverageGuy\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.thatAverageGuy\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-quantization-foundation/01-CONTEXT.md
@.planning/phases/01-core-quantization-foundation/01-RESEARCH.md
@.planning/REQUIREMENTS.md

RESEARCH.md patterns to follow:
- Pattern 2: Dynamic Quantization with Per-Channel Scaling
- Pattern 3: FP16 Quantization (Simple Cast Approach)
- Code Examples: Dynamic INT8 Quantization and FP16 Quantization

CONTEXT.md decisions:
- FP16 approach is thatAverageGuy's discretion (simple casting chosen)
- Per-channel scaling axis=0 for standard PyTorch weight layout
</context>

<tasks>

<task type="auto">
  <name>Create INT8 and FP16 weight quantization functions</name>
  <files>src/mono_quant/core/quantizers.py</files>
  <action>
Create src/mono_quant/core/quantizers.py with:

1. Import torch, copy, Tuple, Optional
2. Import calculate_scale_zp_per_channel from mono_quant.core.mappers

2. Create quantize_weight_int8() function:
   - Parameters: weight (torch.Tensor), symmetric=False, axis=0
   - Calculate scale, zero_point using calculate_scale_zp_per_channel(weight, symmetric=symmetric, axis=axis)
   - Use torch.quantize_per_channel(weight, scale, zero_point.int(), axis=axis, dtype=torch.qint8)
   - Return quantized tensor
   - Include docstring explaining per-channel quantization for standard weight layout

3. Create quantize_weight_fp16() function:
   - Parameters: weight (torch.Tensor)
   - Simply return weight.to(torch.float16)
   - Include docstring referencing RESEARCH.md decision on simple casting vs full pipeline

4. Create dequantize_weight() function:
   - Parameters: q_weight (quantized tensor)
   - Use torch.dequantize() for int8 quantized tensors
   - Use .to(torch.float32) for fp16 tensors
   - Detect type using isinstance or dtype check

Export all functions from src/mono_quant/core/__init__.py
  </action>
  <verify>python -c "from mono_quant.core.quantizers import quantize_weight_int8; import torch; w = torch.randn(64, 128); qw = quantize_weight_int8(w); print(qw.dtype)"</verify>
  <done>Quantization functions transform weights to INT8 or FP16</done>
</task>

<task type="auto">
  <name>Create QuantizedLinear module</name>
  <files>src/mono_quant/modules/linear.py</files>
  <action>
Create src/mono_quant/modules/linear.py with:

1. Import torch, nn, copy
2. Import quantize_weight_int8 from mono_quant.core.quantizers

2. Create QuantizedLinear class (inherits nn.Module):
   - Constructor: __init__(self, in_features, out_features, bias=True, dtype=torch.qint8, symmetric=False)
   - Store in_features, out_features, bias, dtype, symmetric
   - Create weight as nn.Parameter (not quantized yet - placeholder)
   - Create bias as nn.Parameter if bias=True
   - Store _quantized_weight as None (cache for quantized weights)

   - forward() method:
     - If _quantized_weight is None: call self._quantize()
     - Use torch.nn.functional.linear with dequantized weight
     - Add bias if present
     - Return result

   - _quantize() private method:
     - Quantize self.weight using quantize_weight_int8(self.weight.data, symmetric=self.symmetric)
     - Store in self._quantized_weight
     - Delete self.weight.data to free memory (or keep both for flexibility)

3. Create quantize_linear_module() function:
   - Input: nn.Linear module, dtype=torch.qint8, symmetric=False
   - Create new QuantizedLinear with same in_features, out_features, bias
   - Copy quantized weights: use quantize_weight_int8(module.weight.data, symmetric=symmetric)
   - Copy bias if present
   - Return new QuantizedLinear instance

4. Create quantize_conv2d_module() function:
   - Input: nn.Conv2d module, dtype=torch.qint8, symmetric=False
   - Quantize weight per-channel (axis=0 for output channels)
   - Use quantize_weight_int8(module.weight.data, symmetric=symmetric, axis=0)
   - Create new nn.Conv2d with same parameters (in_channels, out_channels, kernel_size, stride, padding, dilation, groups, padding_mode, bias)
   - Copy quantized weights and bias (if present) to new module
   - Return new Conv2d instance with quantized weights
   - DO NOT create placeholder - implement full Conv2d quantization

Export classes and functions from src/mono_quant/modules/__init__.py
  </action>
  <verify>python -c "from mono_quant.modules import QuantizedLinear; import torch.nn as nn; m = nn.Linear(10, 10); print('module created')"</verify>
  <done>QuantizedLinear module wraps quantized weights for inference</done>
</task>

</tasks>

<verification>
Test quantization:
1. quantize_weight_int8 produces torch.qint8 dtype
2. quantize_weight_fp16 produces torch.float16 dtype
3. QuantizedLinear can be instantiated
4. quantize_linear_module returns QuantizedLinear with quantized weights
</verification>

<success_criteria>
- INT8 quantization uses torch.quantize_per_channel with axis=0
- FP16 quantization uses simple .to(torch.float16)
- QuantizedLinear stores quantized weights
- Bias is preserved in quantized modules
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-quantization-foundation/01-03-SUMMARY.md`
</output>
