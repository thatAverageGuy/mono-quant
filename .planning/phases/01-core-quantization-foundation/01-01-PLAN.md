---
phase: 01-core-quantization-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/mono_quant/__init__.py
  - src/mono_quant/config/__init__.py
  - src/mono_quant/config/quant_config.py
  - src/mono_quant/io/__init__.py
  - src/mono_quant/io/handlers.py
  - src/mono_quant/core/__init__.py
autonomous: true

must_haves:
  truths:
    - "Package installs with only torch as required dependency"
    - "QuantizationConfig dataclass exists with dtype, symmetric, per_channel fields"
    - "Input handler accepts both nn.Module and state_dict"
    - "Input handler always copies the model (never modifies original)"
    - "Project structure follows modular architecture from RESEARCH.md"
  artifacts:
    - path: "pyproject.toml"
      provides: "Package configuration with torch-only dependency"
      contains: "torch"
    - path: "src/mono_quant/config/quant_config.py"
      provides: "QuantizationConfig dataclass"
      exports: ["QuantizationConfig"]
    - path: "src/mono_quant/io/handlers.py"
      provides: "Model input handling"
      exports: ["_prepare_model", "_detect_input_format"]
  key_links:
    - from: "src/mono_quant/io/handlers.py"
      to: "copy.deepcopy"
      via: "import"
      pattern: "from copy import deepcopy"
---

<objective>
Set up project structure, package configuration, and model-agnostic input handling. This creates the foundation for all quantization operations by establishing the module structure, configuration system, and input handling that respects the CONTEXT.md requirement to always copy the user's model.

Purpose: Establish clean architecture and ensure quantization never modifies user's original model
Output: Installable package with config system and input handling that accepts nn.Module or state_dict
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-quantization-foundation/01-CONTEXT.md
@.planning/phases/01-core-quantization-foundation/01-RESEARCH.md
@.planning/REQUIREMENTS.md

Key CONTEXT.md decisions:
- Always copy the model - never modify original
- Accept both nn.Module and state_dict
- Configuration priority: function parameters > global config > defaults
</context>

<tasks>

<task type="auto">
  <name>Create project structure and package configuration</name>
  <files>pyproject.toml, src/mono_quant/__init__.py, src/mono_quant/core/__init__.py, src/mono_quant/config/__init__.py, src/mono_quant/io/__init__.py</files>
  <action>
Create pyproject.toml with:
- Project metadata: name="mono-quant", version="0.1.0"
- Build system: setuptools
- Required dependencies: only torch (no HF transformers, no torchao)
- Optional dependencies: typing-extensions for type hints
- Python 3.11+ requirement
- Development deps: pytest, ruff, mypy

Create directory structure:
```
src/mono_quant/
├── __init__.py           # Empty placeholder, exports added in 01-04
├── core/
│   └── __init__.py       # Empty placeholder
├── config/
│   └── __init__.py       # Empty placeholder
└── io/
    └── __init__.py       # Empty placeholder
```

DO NOT add any quantized modules yet (that's 01-03).
DO NOT add public API exports yet (that's 01-04).
  </action>
  <verify>pip install -e . succeeds and imports work: python -c "import mono_quant; print(mono_quant.__file__)"</verify>
  <done>Package is installable, torch is only required dependency, module structure exists</done>
</task>

<task type="auto">
  <name>Create QuantizationConfig dataclass</name>
  <files>src/mono_quant/config/quant_config.py</files>
  <action>
Create src/mono_quant/config/quant_config.py with:

1. Import dataclass, Optional, torch
2. Create QuantizationConfig dataclass with fields:
   - dtype: torch.dtype (default=torch.qint8) - Target quantization dtype
   - symmetric: Optional[bool] (default=None) - None means layer-dependent
   - per_channel: bool (default=True) - Per-channel vs per-tensor scaling
   - bits: int (default=8) - Quantization bit width

3. Add __post_init__ to validate dtype is one of: torch.qint8, torch.float16, torch.qint4 (future)
   Raise ValueError if invalid dtype

4. Add classmethod from_kwargs() to support config priority (kwargs > config > defaults)

Export QuantizationConfig from src/mono_quant/config/__init__.py
  </action>
  <verify>python -c "from mono_quant.config import QuantizationConfig; c = QuantizationConfig(); assert c.dtype == torch.qint8"</verify>
  <done>QuantizationConfig dataclass exists with proper validation and defaults</done>
</task>

<task type="auto">
  <name>Create model-agnostic input handlers</name>
  <files>src/mono_quant/io/handlers.py</files>
  <action>
Create src/mono_quant/io/handlers.py with:

1. Import copy.deepcopy, torch, nn, Union, Dict, Tuple

2. Create _detect_input_format() function:
   - Input: Union[nn.Module, Dict[str, torch.Tensor]]
   - Returns: str "module" or "state_dict"
   - Use isinstance checks
   - Raise TypeError if neither type

3. Create _prepare_model() function:
   - Input: Union[nn.Module, Dict[str, torch.Tensor]]
   - Returns: nn.Module (copied)
   - If nn.Module: return deepcopy(model) to preserve original
   - If state_dict: raise NotImplementedError with helpful message explaining state_dict support requires architecture info (Phase 1 limitation per RESEARCH.md)
   - Include docstring documenting why copy is required (CONTEXT.md decision)

4. Create _validate_model() function:
   - Input: nn.Module
   - Returns: List[str] of layer names that are quantizable (Linear, Conv2d)
   - Use isinstance checks for nn.Linear, nn.Conv2d
   - Return list of (name, module_type) tuples

Export all functions from src/mono_quant/io/__init__.py
  </action>
  <verify>python -c "from mono_quant.io import _prepare_model, _detect_input_format; import torch.nn as nn; m = nn.Linear(10, 10); assert id(m) != id(_prepare_model(m))"</verify>
  <done>Input handlers detect format, copy models, and identify quantizable layers</done>
</task>

</tasks>

<verification>
Install package and verify:
1. torch is only runtime dependency (check pip show)
2. QuantizationConfig can be instantiated with defaults
3. _prepare_model creates a copy (id() != original id)
4. _detect_input_format correctly identifies nn.Module vs state_dict
</verification>

<success_criteria>
- Package installs with `pip install -e .`
- Only torch required (pip check shows no missing deps)
- All imports work: from mono_quant.config import QuantizationConfig; from mono_quant.io import _prepare_model
- Model copying preserves original (test shows id difference)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-quantization-foundation/01-01-SUMMARY.md`
</output>
