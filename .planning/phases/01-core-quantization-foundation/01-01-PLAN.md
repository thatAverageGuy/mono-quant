---
phase: 01-core-quantization-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/mono_quant/__init__.py
  - src/mono_quant/config/__init__.py
  - src/mono_quant/config/quant_config.py
  - src/mono_quant/io/__init__.py
  - src/mono_quant/io/handlers.py
  - src/mono_quant/core/__init__.py
autonomous: true

must_haves:
  truths:
    - "Package installs with only torch as required dependency"
    - "QuantizationConfig dataclass exists with dtype, symmetric, per_channel fields"
    - "Input handler accepts both nn.Module and state_dict (AGN-02)"
    - "Input handler loads state_dict when architecture is provided"
    - "Input handler raises helpful error for state_dict without architecture"
    - "Input handler always copies the model (never modifies original)"
    - "Project structure follows modular architecture from RESEARCH.md"
  artifacts:
    - path: "pyproject.toml"
      provides: "Package configuration with torch-only dependency"
      contains: "torch"
    - path: "src/mono_quant/config/quant_config.py"
      provides: "QuantizationConfig dataclass"
      exports: ["QuantizationConfig"]
    - path: "src/mono_quant/io/handlers.py"
      provides: "Model input handling with state_dict support"
      exports: ["_prepare_model", "_detect_input_format"]
      contains: "def _prepare_model"
      contains: "def _detect_input_format"
  key_links:
    - from: "src/mono_quant/io/handlers.py"
      to: "copy.deepcopy"
      via: "import"
      pattern: "from copy import deepcopy"
    - from: "src/mono_quant/io/handlers.py"
      to: "state_dict loading"
      via: "model.load_state_dict()"
      pattern: "load_state_dict"
---

<objective>
Set up project structure, package configuration, and model-agnostic input handling. This creates the foundation for all quantization operations by establishing the module structure, configuration system, and input handling that respects the CONTEXT.md requirement to always copy the user's model.

Purpose: Establish clean architecture and ensure quantization never modifies user's original model
Output: Installable package with config system and input handling that accepts nn.Module or state_dict
</objective>

<execution_context>
@C:\Users\ghost\.thatAverageGuy\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.thatAverageGuy\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-quantization-foundation/01-CONTEXT.md
@.planning/phases/01-core-quantization-foundation/01-RESEARCH.md
@.planning/REQUIREMENTS.md

Key CONTEXT.md decisions:
- Always copy the model - never modify original
- Accept both nn.Module and state_dict
- Configuration priority: function parameters > global config > defaults
</context>

<tasks>

<task type="auto">
  <name>Create project structure and package configuration</name>
  <files>pyproject.toml, src/mono_quant/__init__.py, src/mono_quant/core/__init__.py, src/mono_quant/config/__init__.py, src/mono_quant/io/__init__.py</files>
  <action>
Create pyproject.toml with:
- Project metadata: name="mono-quant", version="0.1.0"
- Build system: setuptools
- Required dependencies: only torch (no HF transformers, no torchao)
- Optional dependencies: typing-extensions for type hints
- Python 3.11+ requirement
- Development deps: pytest, ruff, mypy

Create directory structure:
```
src/mono_quant/
├── __init__.py           # Empty placeholder, exports added in 01-04
├── core/
│   └── __init__.py       # Empty placeholder
├── config/
│   └── __init__.py       # Empty placeholder
└── io/
    └── __init__.py       # Empty placeholder
```

DO NOT add any quantized modules yet (that's 01-03).
DO NOT add public API exports yet (that's 01-04).
  </action>
  <verify>pip install -e . succeeds and imports work: python -c "import mono_quant; print(mono_quant.__file__)"</verify>
  <done>Package is installable, torch is only required dependency, module structure exists</done>
</task>

<task type="auto">
  <name>Create QuantizationConfig dataclass</name>
  <files>src/mono_quant/config/quant_config.py</files>
  <action>
Create src/mono_quant/config/quant_config.py with:

1. Import dataclass, Optional, torch
2. Create QuantizationConfig dataclass with fields:
   - dtype: torch.dtype (default=torch.qint8) - Target quantization dtype
   - symmetric: Optional[bool] (default=None) - None means layer-dependent
   - per_channel: bool (default=True) - Per-channel vs per-tensor scaling
   - bits: int (default=8) - Quantization bit width

3. Add __post_init__ to validate dtype is one of: torch.qint8, torch.float16, torch.qint4 (future)
   Raise ValueError if invalid dtype

4. Add classmethod from_kwargs() to support config priority (kwargs > config > defaults)

Export QuantizationConfig from src/mono_quant/config/__init__.py
  </action>
  <verify>python -c "from mono_quant.config import QuantizationConfig; c = QuantizationConfig(); assert c.dtype == torch.qint8"</verify>
  <done>QuantizationConfig dataclass exists with proper validation and defaults</done>
</task>

<task type="auto">
  <name>Create model-agnostic input handlers</name>
  <files>src/mono_quant/io/handlers.py</files>
  <action>
Create src/mono_quant/io/handlers.py with:

1. Import copy.deepcopy, torch, nn, Union, Dict, Tuple

2. Create _detect_input_format() function:
   - Input: Union[nn.Module, Dict[str, torch.Tensor]]
   - Returns: str "module" or "state_dict"
   - Use isinstance checks (nn.Module for modules, dict for state_dict)
   - Raise TypeError if neither type with helpful message

3. Create _prepare_model() function (AGN-02 compliance):
   - Input: Union[nn.Module, Dict[str, torch.Tensor]], architecture: Optional[Type[nn.Module]] = None
   - Returns: nn.Module (copied)
   - If nn.Module: return deepcopy(model) to preserve original (CONTEXT.md requirement)
   - If state_dict:
     - If architecture is None: raise ValueError with helpful message explaining state_dict requires architecture info
     - If architecture provided: instantiate architecture(), load state_dict, return copy
   - Include docstring documenting both input paths and why copy is required (CONTEXT.md decision)

4. Create _validate_model() function:
   - Input: nn.Module
   - Returns: List[Tuple[str, str]] of (layer_name, layer_type) for quantizable layers
   - Use isinstance checks for nn.Linear, nn.Conv2d
   - Return list of tuples for quantization pipeline

Export all functions from src/mono_quant/io/__init__.py
  </action>
  <verify>
  1. python -c "from mono_quant.io import _prepare_model, _detect_input_format; import torch.nn as nn; m = nn.Linear(10, 10); assert id(m) != id(_prepare_model(m))" (verifies copy)
  2. python -c "from mono_quant.io import _detect_input_format, _prepare_model; import torch; sd = {'weight': torch.randn(10,10)}; assert _detect_input_format(sd) == 'state_dict'" (verifies state_dict detection)
  </verify>
  <done>Input handlers detect both formats, copy models, load state_dict with architecture, and identify quantizable layers</done>
</task>

</tasks>

<verification>
Install package and verify:
1. torch is only runtime dependency (check pip show)
2. QuantizationConfig can be instantiated with defaults
3. _prepare_model creates a copy (id() != original id)
4. _detect_input_format correctly identifies nn.Module vs state_dict
</verification>

<success_criteria>
- Package installs with `pip install -e .`
- Only torch required (pip check shows no missing deps)
- All imports work: from mono_quant.config import QuantizationConfig; from mono_quant.io import _prepare_model
- Model copying preserves original (test shows id difference)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-quantization-foundation/01-01-SUMMARY.md`
</output>
