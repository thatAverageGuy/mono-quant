---
phase: 04-user-interfaces
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/mono_quant/api/__init__.py
  - src/mono_quant/api/quantize.py
  - src/mono_quant/api/result.py
  - src/mono_quant/api/exceptions.py
  - src/mono_quant/__init__.py
  - pyproject.toml
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can quantize via unified Python API: quantize(model, bits=8, dynamic=False)"
    - "API accepts nn.Module, state_dict, or file path as input"
    - "QuantizationResult provides .model, .info, .success, .errors, .warnings"
    - "Result object has convenience methods: .save(), .validate()"
    - "Common parameters exposed: bits, scheme, dynamic, calibration_data"
    - "Advanced options available via **kwargs (group_size, observer_type, etc.)"
    - "Exceptions include actionable suggestions for common errors"
  artifacts:
    - path: "src/mono_quant/api/quantize.py"
      provides: "Unified quantize() function dispatching to dynamic/static"
      min_lines: 150
      exports: ["quantize"]
    - path: "src/mono_quant/api/result.py"
      provides: "QuantizationResult dataclass with convenience methods"
      min_lines: 80
      exports: ["QuantizationResult"]
    - path: "src/mono_quant/api/exceptions.py"
      provides: "Custom exception hierarchy with actionable messages"
      min_lines: 50
      exports: ["MonoQuantError", "QuantizationError", "ValidationError", "ConfigurationError"]
    - path: "src/mono_quant/__init__.py"
      provides: "Public API exports including quantize()"
      contains: "from mono_quant.api import quantize"
  key_links:
    - from: "src/mono_quant/api/quantize.py"
      to: "src/mono_quant/core/quantizers.py"
      via: "quantize() calls dynamic_quantize() or static_quantize()"
      pattern: "dynamic_quantize|static_quantize"
    - from: "src/mono_quant/api/result.py"
      to: "src/mono_quant/io/formats.py"
      via: ".save() calls save_model()"
      pattern: "save_model"
    - from: "src/mono_quant/api/quantize.py"
      to: "src/mono_quant/io/handlers.py"
      via: "Input normalization via _prepare_model()"
      pattern: "_prepare_model|load_model"
---

<objective>
Build a unified Python API that simplifies quantization workflows by providing a single `quantize()` function that dispatches to `dynamic_quantize()` or `static_quantize()` based on parameters. The API returns a `QuantizationResult` object with both direct attribute access and convenience methods for common operations.

Purpose: Simplify the user-facing API by providing a single entry point for quantization that handles dynamic/static dispatch, input format normalization, and provides a rich result object with success/failure tracking and convenience methods. This wraps existing quantization functionality (Phases 1-3) without introducing new quantization algorithms.

Output: Unified `quantize()` function in `src/mono_quant/api/`, `QuantizationResult` dataclass with `.save()` and `.validate()` methods, custom exception hierarchy with actionable error messages, and updated public API exports.
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-user-interfaces/04-CONTEXT.md
@.planning/phases/04-user-interfaces/04-RESEARCH.md
@.planning/phases/03-advanced-calibration-&-int4/03-02-SUMMARY.md
@.planning/phases/02-static-quantization-&-i/02-02-SUMMARY.md
@.planning/phases/01-core-quantization-foundation/01-04-SUMMARY.md

# Existing APIs to Wrap
- src/mono_quant/core/quantizers.py: dynamic_quantize() returns (model, skipped), static_quantize() returns (model, info)
- src/mono_quant/io/__init__.py: save_model(), load_model() for file I/O
- src/mono_quant/io/validation.py: validate_quantization() for validation
- src/mono_quant/io/handlers.py: _prepare_model() for model copying and normalization

# Key Implementation Notes from CONTEXT.md
- Unified function approach: Single quantize() dispatching based on parameters
- Parameter handling: Common options exposed (bits, scheme, dynamic), advanced via **kwargs
- Return value: QuantizationResult with .model, .info, .success, .errors, .save(), .validate()
- Input flexibility: Accept nn.Module, state_dict, or file path

# Key Implementation Notes from RESEARCH.md
- Use dataclass for QuantizationResult (built-in, type-safe, zero dependencies)
- Use explicit common parameters with **kwargs for advanced options
- Hybrid error approach: exceptions raise, but Result also has .success flag
- show_progress parameter (default False) for Python API progress bars
- tqdm for progress reporting (minimal deps, CI-friendly)

# Decisions from Context (LOCKED)
- CLI command name: monoquant (primary), mq (alias documented but secondary)
- Python API progress: show_progress parameter, default False (silent for library use)
- bits parameter: 4, 8, or 16 (for FP16) - maps to dtype internally
- scheme parameter: "symmetric" or "asymmetric" (string, not enum)
- dynamic flag: controls dispatch to dynamic_quantize vs static_quantize
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create custom exception hierarchy</name>
  <files>src/mono_quant/api/exceptions.py</files>
  <action>
Create src/mono_quant/api/exceptions.py with custom exception hierarchy:

1. MonoQuantError(Exception) - Base exception
   - Attributes: message (str), suggestion (Optional[str])
   - __init__(self, message: str, suggestion: Optional[str] = None)
   - Build full_message from message + suggestion if provided
   - Call super().__init__(full_message)

2. QuantizationError(MonoQuantError) - Quantization failures
   - Raised when quantization fails (e.g., group_size too large for layer)

3. ValidationError(MonoQuantError) - Validation failures
   - Raised when model validation fails

4. ConfigurationError(MonoQuantError) - Invalid configuration
   - Raised when parameters are invalid

5. InputError(MonoQuantError) - Invalid input
   - Raised when model input is not recognized

Example error with actionable suggestion:
  raise QuantizationError(
      "Layer dimension 64 is smaller than group_size 128",
      suggestion="Use --group-size 64 or smaller for this model"
  )

Add comprehensive docstrings for each exception class with usage examples.
</action>
  <verify>
from mono_quant.api.exceptions import QuantizationError, ConfigurationError

# Test exception creation with suggestion
try:
    raise QuantizationError(
        "Layer too small",
        suggestion="Reduce group_size to 64"
    )
except QuantizationError as e:
    assert "Layer too small" in str(e)
    assert "Suggestion:" in str(e) or "64" in str(e)

# Test exception without suggestion
try:
    raise ConfigurationError("Invalid bits value")
except ConfigurationError as e:
    assert "Invalid bits value" in str(e)
</verify>
  <done>
Custom exception hierarchy created with MonoQuantError base class and specific exception types (QuantizationError, ValidationError, ConfigurationError, InputError). All exceptions support actionable suggestions via the suggestion parameter.
</done>
</task>

<task type="auto">
  <name>Task 2: Create QuantizationResult dataclass</name>
  <files>src/mono_quant/api/result.py</files>
  <action>
Create src/mono_quant/api/result.py with QuantizationResult dataclass:

1. QuantizationResult dataclass with fields:
   - model: nn.Module (the quantized model)
   - info: QuantizationInfo (metadata from quantization)
   - success: bool = True (indicates if quantization succeeded)
   - errors: List[str] = field(default_factory=list) (error messages)
   - warnings: List[str] = field(default_factory=list) (warning messages)

2. Convenience methods:

   a. save(self, path: Union[str, Path]) -> None
      - Import save_model from mono_quant.io (local import)
      - Call save_model(self.model, path, quantization_info=self.info)
      - No return value (saves to disk)

   b. validate(self, on_failure: str = "error") -> ValidationResult
      - Import validate_quantization from mono_quant.io.validation (local import)
      - Needs original model reference - for now, raise NotImplementedError
        with message to use validate_quantization directly
      - This is a placeholder for future enhancement

   c. __bool__(self) -> bool
      - Return self.success
      - Allows: if result: ... pattern

3. __post_init__ to handle success flag:
   - If errors list is non-empty, set success = False

Add comprehensive docstrings with examples showing usage patterns.
</action>
  <verify>
from mono_quant.api.result import QuantizationResult
from mono_quant.core.quantizers import QuantizationInfo
import torch.nn as nn

# Test basic result creation
model = nn.Linear(10, 20)
info = QuantizationInfo(
    selected_layers=["0"],
    skipped_layers=[],
    calibration_samples_used=100,
    dtype=torch.int32,
    symmetric=False,
)
result = QuantizationResult(model=model, info=info)
assert result.success is True
assert bool(result) is True

# Test with errors
result_with_error = QuantizationResult(
    model=model,
    info=info,
    errors=["Layer xyz failed"]
)
assert result_with_error.success is False
assert bool(result_with_error) is False

# Test __bool__ override
if result:
    passed = True
else:
    passed = False
assert passed is True
</verify>
  <done>
QuantizationResult dataclass created with model, info, success, errors, and warnings fields. Convenience methods include .save() for saving quantized models, .validate() placeholder, and __bool__ override for truthiness checks. __post_init__ automatically sets success=False when errors exist.
</done>
</task>

<task type="auto">
  <name>Task 3: Create unified quantize() function</name>
  <files>src/mono_quant/api/quantize.py</files>
  <action>
Create src/mono_quant/api/quantize.py with unified quantize() function:

1. quantize() function with signature:
   def quantize(
       model: Union[nn.Module, Dict[str, torch.Tensor], str, Path],
       bits: int = 8,
       dynamic: bool = False,
       scheme: str = "symmetric",
       calibration_data: Optional[CalibrationData] = None,
       show_progress: bool = False,
       **kwargs
   ) -> QuantizationResult:

   Implementation:
   - Import types: Union, Optional, List, Dict, Path
   - Import torch, torch.nn as nn
   - Local imports: dynamic_quantize, static_quantize from mono_quant.core.quantizers
   - Local imports: _prepare_model, load_model from mono_quant.io.handlers
   - Local imports: QuantizationResult from .result
   - Local imports: QuantizationError, InputError, ConfigurationError from .exceptions

2. Input normalization:
   - If model is str or Path: load_model(str(model)) to get state_dict
   - Call _prepare_model() to get nn.Module copy (handles both nn.Module and dict)

3. Parameter validation:
   - Validate bits in [4, 8, 16]: else raise ConfigurationError
   - Map bits to dtype: {4: torch.qint8, 8: torch.qint8, 16: torch.float16}
   - Validate scheme in ["symmetric", "asymmetric"]: else raise ConfigurationError
   - For static quantization (not dynamic): require calibration_data else raise InputError

4. Dispatch to appropriate function:
   - If dynamic:
     - q_model, skipped = dynamic_quantize(model, dtype=dtype, symmetric=(scheme=="symmetric"), **kwargs)
     - Create QuantizationInfo with selected_layers=[], skipped_layers=skipped, calibration_samples_used=0
   - Else (static):
     - q_model, info = static_quantize(model, calibration_data=calibration_data, dtype=dtype, symmetric=(scheme=="symmetric"), **kwargs)
     - Use returned info directly

5. Build result:
   - Create QuantizationResult(model=q_model, info=info, success=True)
   - Copy warnings from info.warnings to result.warnings

6. Error handling:
   - Wrap quantization calls in try/except
   - On exception: return QuantizationResult with success=False, errors=[str(e)]

7. show_progress handling:
   - Pass show_progress via **kwargs to underlying functions if they support it
   - Note: Phase 4-02 will add progress bar support to calibration

Add comprehensive docstring with examples for:
- Dynamic quantization: quantize(model, bits=8, dynamic=True)
- Static quantization: quantize(model, bits=8, calibration_data=data)
- From file path: quantize("model.pt", bits=8, dynamic=True)
</action>
  <verify>
from mono_quant.api.quantize import quantize
import torch.nn as nn

# Test dynamic quantization
model = nn.Sequential(nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 10))
result = quantize(model, bits=8, dynamic=True)
assert result.success is True
assert isinstance(result.model, nn.Sequential)
assert hasattr(result, 'info')

# Test with invalid bits (should fail gracefully)
try:
    result = quantize(model, bits=5)
    assert False, "Should have raised error"
except Exception:
    pass  # Expected

# Test with file path (will fail if file doesn't exist, but tests input handling)
try:
    result = quantize("nonexistent.pt", bits=8, dynamic=True)
except Exception:
    pass  # File doesn't exist, but input handling works
</verify>
  <done>
Unified quantize() function created that accepts nn.Module, state_dict, or file path as input. Dispatches to dynamic_quantize() or static_quantize() based on dynamic flag. Maps bits (4/8/16) to dtype, validates scheme parameter, requires calibration_data for static quantization. Returns QuantizationResult with model, info, and success flag.
</done>
</task>

<task type="auto">
  <name>Task 4: Update package exports and dependencies</name>
  <files>src/mono_quant/api/__init__.py</files>
  <action>
Create src/mono_quant/api/__init__.py to export the public API:

1. Import and export from quantize module:
   - from .quantize import quantize
   - from .result import QuantizationResult
   - from .exceptions import (
       MonoQuantError,
       QuantizationError,
       ValidationError,
       ConfigurationError,
       InputError,
     )

2. Define __all__ list:
   - "quantize"
   - "QuantizationResult"
   - "MonoQuantError"
   - "QuantizationError"
   - "ValidationError"
   - "ConfigurationError"
   - "InputError"

3. Add module docstring explaining the unified API

Update src/mono_quant/__init__.py:
1. Add import: from mono_quant.api import quantize
2. Add "quantize" to __all__ list (after "static_quantize")
3. Update module docstring to mention unified quantize() function

Update pyproject.toml:
1. Add tqdm to dependencies (not dev dependencies) since CLI will need it
2. Click dependency will be added in plan 04-02 (CLI)
3. Move tqdm from [project.optional-dependencies] dev to main dependencies list

Current dependencies should become:
  dependencies = [
      "torch>=2.0.0",
      "safetensors>=0.4",
      "tqdm>=4.66",
  ]
</action>
  <verify>
# Test imports from api module
from mono_quant.api import quantize, QuantizationResult
from mono_quant.api import QuantizationError, ConfigurationError

# Verify quantize in api.__all__
from mono_quant import api
assert "quantize" in api.__all__
assert "QuantizationResult" in api.__all__

# Verify quantize available from top-level
from mono_quant import quantize
assert callable(quantize)

# Check pyproject.toml has tqdm in dependencies
import tomli
with open("pyproject.toml", "rb") as f:
    data = tomli.load(f)
    deps = data["project"]["dependencies"]
    assert any("tqdm" in dep for dep in deps)
</verify>
  <done>
API module exports unified quantize() function, QuantizationResult, and exception types. Top-level __init__.py updated to export quantize() in public API. pyproject.toml updated to include tqdm as a main dependency (required for progress bars in both Python API and CLI).
</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. from mono_quant import quantize - imports successfully
2. from mono_quant.api import QuantizationResult, QuantizationError - imports successfully
3. quantize(model, bits=8, dynamic=True) returns QuantizationResult with success=True
4. quantize(model, bits=8, dynamic=False, calibration_data=data) returns QuantizationResult with info populated
5. QuantizationResult has .save() method that saves to disk
6. QuantizationResult.__bool__ returns True when success=True
7. Exception classes include suggestion parameter for actionable messages
8. bits=4,8,16 map to correct dtypes (qint8, qint8, float16)
9. Invalid bits/scheme values raise ConfigurationError
10. Static quantization without calibration_data raises InputError
</verification>

<success_criteria>
- User can call quantize(model, bits=8, dynamic=False) for simplified quantization
- quantize() accepts nn.Module, state_dict, or file path as input
- QuantizationResult provides .model, .info, .success attributes and .save() convenience method
- Common parameters (bits, scheme, dynamic, calibration_data) exposed at top level
- Advanced options (group_size, observer_type, etc.) available via **kwargs
- Custom exceptions include actionable suggestions for common errors
- tqdm added to main dependencies for progress reporting
</success_criteria>

<output>
After completion, create `.planning/phases/04-user-interfaces/04-01-SUMMARY.md`
</output>
