---
phase: 03-advanced-calibration-&-int4
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/mono_quant/core/observers.py
  - src/mono_quant/core/__init__.py
  - src/mono_quant/calibration/runner.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can select MovingAverageMinMaxObserver for robust calibration with outlier handling"
    - "User can select HistogramObserver for outlier-aware calibration using KL divergence"
    - "MovingAverageMinMaxObserver uses exponential moving average with configurable averaging_constant"
    - "HistogramObserver builds histogram and selects optimal threshold via KL divergence minimization"
    - "Observers follow same interface as MinMaxObserver (forward, calculate_qparams, reset)"
    - "Observers avoid deprecated torch.ao.quantization APIs (custom implementations)"
  artifacts:
    - path: "src/mono_quant/core/observers.py"
      provides: "MovingAverageMinMaxObserver and HistogramObserver classes"
      min_lines: 200
      exports: ["MovingAverageMinMaxObserver", "HistogramObserver"]
    - path: "src/mono_quant/calibration/runner.py"
      provides: "Observer factory and auto-selection logic"
      contains: "create_observer|_auto_select_observer"
  key_links:
    - from: "src/mono_quant/calibration/runner.py"
      to: "src/mono_quant/core/observers.py"
      via: "Observer factory creates observer instances by name"
      pattern: "create_observer.*MinMax|MovingAverage|Histogram"
    - from: "src/mono_quant/core/observers.py"
      to: "src/mono_quant/core/observers.py:MinMaxObserver"
      via: "All observers share common interface (forward, calculate_qparams, reset)"
      pattern: "class.*Observer.*def forward.*def calculate_qparams.*def reset"
---

<objective>
Build advanced calibration observers (MovingAverageMinMax and Histogram) for robust calibration in scenarios with outliers or non-uniform data distributions. Both observers follow the same interface as MinMaxObserver for easy substitution in calibration code.

Purpose: MinMaxObserver is sensitive to outliers - single extreme values can distort the entire quantization range. MovingAverageMinMaxObserver smooths out transient spikes using exponential moving average. HistogramObserver uses KL divergence minimization to find optimal quantization thresholds, making it robust to skewed distributions.

Output: MovingAverageMinMaxObserver with configurable averaging_constant (default 0.01 per PyTorch standard), HistogramObserver with KL divergence minimization, observer factory for easy instantiation, and auto-selection logic (marked as experimental).
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-advanced-calibration-&-int4/03-CONTEXT.md
@.planning/phases/03-advanced-calibration-&-int4/03-RESEARCH.md
@.planning/phases/02-static-quantization-&-i/02-01-SUMMARY.md

# Context from Phase 1-2
- Existing: src/mono_quant/core/observers.py (MinMaxObserver with forward/calculate_qparams/reset interface)
- Existing: src/mono_quant/calibration/runner.py (run_calibration function)
- Pattern: Custom observers to avoid deprecated torch.ao.quantization APIs
- Pattern: All observers share same interface for substitutability

# Key Implementation Notes from RESEARCH.md
- MovingAverageMinMaxObserver:
  - Formula: min_new = (1 - c) * min_old + c * min_current
  - averaging_constant default: 0.01 (PyTorch standard)
  - Lower (0.001) = more stable, Higher (0.1) = more responsive
- HistogramObserver:
  - Uses KL divergence (entropy) minimization for threshold selection
  - Similar approach to TensorRT quantization
  - Default bins: 2048
  - Computationally more expensive than MinMax
- Auto-selection: Mark as experimental (unreliable heuristics)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MovingAverageMinMaxObserver</name>
  <files>src/mono_quant/core/observers.py</files>
  <action>
Add MovingAverageMinMaxObserver class to src/mono_quant/core/observers.py:

1. MovingAverageMinMaxObserver class

   Attributes:
   - averaging_constant: float (weight for new observations, default 0.01)
   - dtype: torch.dtype (target quantization dtype)
   - min_val: Optional[float] (EMA of minimum values)
   - max_val: Optional[float] (EMA of maximum values)

   Methods:

   a. __init__(
        self,
        averaging_constant: float = 0.01,
        dtype: torch.dtype = torch.qint8,
      )
      - Validate: 0 < averaging_constant <= 1
      - Store averaging_constant, dtype
      - Initialize min_val = None, max_val = None

   b. forward(self, x: torch.Tensor) -> None
      - Extract min/max from tensor: x_min = x.amin().item(), x_max = x.amax().item()
      - If first observation (min_val is None):
        - min_val = x_min, max_val = x_max
      - Else:
        - Apply exponential moving average:
          - c = self.averaging_constant
          - min_val = (1 - c) * min_val + c * x_min
          - max_val = (1 - c) * max_val + c * x_max

   c. calculate_qparams(self) -> Tuple[torch.Tensor, torch.Tensor]
      - Raise RuntimeError if min_val is None (no data observed)
      - Use same calculation as MinMaxObserver:
        - qmin, qmax = -128, 127 (INT8 range)
        - range_val = max_val - min_val
        - q_range = qmax - qmin
        - scale = range_val / q_range
        - scale = max(scale, 1e-8) (clamp to avoid division by zero)
        - zero_point = qmin - (min_val / scale)
        - zero_point = int(round(zero_point))
        - zero_point = max(qmin, min(qmax, zero_point)) (clamp to valid range)
      - Return torch.tensor(scale), torch.tensor(zero_point, dtype=torch.int32)

   d. reset(self) -> None
      - Reset min_val = None, max_val = None

2. Add comprehensive docstring

   Explain:
   - EMA formula for smoothing
   - averaging_constant behavior (lower = stable, higher = responsive)
   - When to use (outliers, transient spikes)
   - Default value matches PyTorch standard (0.01)
</action>
  <verify>
import torch
from mono_quant.core.observers import MovingAverageMinMaxObserver

# Test basic functionality
obs = MovingAverageMinMaxObserver(averaging_constant=0.01)
assert obs.min_val is None

# First observation
x1 = torch.randn(32, 64) * 0.1 + 1.0  # Small variance around 1.0
obs.forward(x1)
assert obs.min_val is not None

# Second observation
x2 = torch.randn(32, 64) * 0.1 + 1.0
obs.forward(x2)

# EMA should smooth values
scale, zp = obs.calculate_qparams()
assert scale.ndim == 0 and zp.ndim == 0

# Test averaging_constant validation
try:
    MovingAverageMinMaxObserver(averaging_constant=0.0)
    assert False, "Should raise ValueError"
except ValueError:
    pass

# Test reset
obs.reset()
assert obs.min_val is None
</verify>
  <done>
MovingAverageMinMaxObserver uses exponential moving average to smooth out transient spikes in calibration data. Configurable averaging_constant (default 0.01 per PyTorch standard) controls responsiveness vs stability. Shares same interface as MinMaxObserver.
</done>
</task>

<task type="auto">
  <name>Task 2: Create HistogramObserver</name>
  <files>src/mono_quant/core/observers.py</files>
  <action>
Add HistogramObserver class to src/mono_quant/core/observers.py:

1. Add imports at top of file (if not present):
   - import math (for entropy calculation)

2. HistogramObserver class

   Attributes:
   - bins: int (number of histogram bins, default 2048)
   - dtype: torch.dtype (target quantization dtype)
   - histogram_counts: Optional[torch.Tensor] (accumulated bin counts)
   - bin_edges: Optional[torch.Tensor] (bin edge values)
   - min_val: Optional[float] (global minimum observed)
   - max_val: Optional[float] (global maximum observed)

   Methods:

   a. __init__(
        self,
        bins: int = 2048,
        dtype: torch.dtype = torch.qint8,
      )
      - Store bins, dtype
      - Initialize histogram_counts = None, bin_edges = None
      - Initialize min_val = None, max_val = None

   b. forward(self, x: torch.Tensor) -> None
      - Track global min/max:
        - x_min = x.amin().item(), x_max = x.amax().item()
        - Update min_val, max_val
      - Build histogram for current tensor:
        - new_counts, new_edges = torch.histogram(x.flatten(), bins=self.bins)
      - Accumulate histograms:
        - If histogram_counts is None:
          - histogram_counts = new_counts
          - bin_edges = new_edges
        - Else:
          - histogram_counts += new_counts
          - (bin_edges stay consistent across calls)

   c. calculate_qparams(self) -> Tuple[torch.Tensor, torch.Tensor]
      - Raise RuntimeError if histogram_counts is None (no data observed)
      - Find optimal threshold using KL divergence minimization:
        - Get distribution P from histogram (normalize to sum=1)
        - For candidate thresholds from 50% to 100% of range:
          - Quantize distribution: Q = quantize_histogram(P, threshold)
          - Compute KL divergence: D_KL(P || Q) = sum(P * log(P / Q))
          - Track threshold with minimum KL divergence
      - Use optimal threshold to determine min/max for quantization
      - Calculate scale and zero_point (same as MinMaxObserver)
      - Return torch.tensor(scale), torch.tensor(zero_point, dtype=torch.int32)

   d. _compute_kl_divergence(
        self,
        P: torch.Tensor,
        Q: torch.Tensor,
      ) -> float
      - Helper to compute KL(P || Q)
      - Add epsilon to avoid log(0)
      - return torch.sum(P * torch.log(P / (Q + 1e-10))).item()

   e. _quantize_distribution(
        self,
        P: torch.Tensor,
        threshold: float,
      ) -> torch.Tensor
      - Helper to create quantized distribution Q
      - Map values outside threshold to boundary bins
      - Normalize to sum=1

   f. reset(self) -> None
      - Reset histogram_counts = None, bin_edges = None
      - Reset min_val = None, max_val = None

3. Add comprehensive docstring

   Explain:
   - KL divergence minimization for threshold selection
   - Similar approach to TensorRT quantization
   - Computationally more expensive than MinMax
   - Best for data with outliers or skewed distributions
   - Default bins=2048 provides good resolution
</action>
  <verify>
import torch
from mono_quant.core.observers import HistogramObserver

# Test basic functionality
obs = HistogramObserver(bins=2048)
assert obs.histogram_counts is None

# First observation
x1 = torch.randn(1000) * 0.5 + 2.0  # Normal distribution around 2.0
obs.forward(x1)
assert obs.histogram_counts is not None
assert obs.min_val is not None

# Second observation (accumulate histogram)
x2 = torch.randn(1000) * 0.5 + 2.0
obs.forward(x2)
assert obs.histogram_counts.sum() > 1000  # Accumulated

# Calculate qparams
scale, zp = obs.calculate_qparams()
assert scale.ndim == 0 and zp.ndim == 0
assert scale > 0

# Test reset
obs.reset()
assert obs.histogram_counts is None
assert obs.min_val is None
</verify>
  <done>
HistogramObserver builds histogram of activation values and selects optimal quantization threshold using KL divergence minimization. More computationally expensive than MinMax but robust to outliers and skewed distributions. Similar approach to TensorRT quantization.
</done>
</task>

<task type="auto">
  <name>Task 3: Create observer factory and auto-selection</name>
  <files>src/mono_quant/calibration/runner.py</files>
  <action>
Add observer factory and auto-selection logic to src/mono_quant/calibration/runner.py:

1. Import observer classes (local import at function level to avoid circular dependency)

2. create_observer(
     observer_type: str,
     **kwargs
   ) -> Union[MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver]

   Implementation:
   - Local import: from mono_quant.core.observers import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver
   - Normalize observer_type: observer_type = observer_type.lower().replace("_", "").replace("-", "")
   - Match against observer types:
     - "minmax", "minmaxobserver": return MinMaxObserver(**kwargs)
     - "movingaverage", "movingaverageminmax", "ema": return MovingAverageMinMaxObserver(**kwargs)
     - "histogram", "histogramobserver", "kl": return HistogramObserver(**kwargs)
     - "auto": return _auto_select_observer(**kwargs)
   - Raise ValueError for unknown observer_type

3. _auto_select_observer(
     **kwargs
   ) -> Union[MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver]

   Implementation:
   - This is marked as EXPERIMENTAL per RESEARCH.md
   - Extract relevant kwargs: averaging_constant, bins
   - Use simple heuristic based on available data:
     - If averaging_constant is specified: use MovingAverageMinMaxObserver
     - If bins is specified: use HistogramObserver
     - Else: use MinMaxObserver (safe default)
   - Add docstring warning: "EXPERIMENTAL: Auto-selection is unreliable. Manual selection recommended."

   Note: More sophisticated auto-selection (dataset size, distribution analysis) is deferred to future work per RESEARCH.md "Open Questions" section.

4. Update run_calibration docstring

   Mention observer parameter and observer factory usage.

Add comprehensive docstrings explaining:
- Observer factory pattern for easy instantiation by string name
- Auto-selection is experimental and uses simple heuristics
- Users should manually select observer based on their data characteristics
</action>
  <verify>
from mono_quant.calibration.runner import create_observer, _auto_select_observer
from mono_quant.core.observers import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver

# Test factory with different observer types
obs1 = create_observer("MinMax")
assert isinstance(obs1, MinMaxObserver)

obs2 = create_observer("MovingAverage")
assert isinstance(obs2, MovingAverageMinMaxObserver)

obs3 = create_observer("Histogram")
assert isinstance(obs3, HistogramObserver)

# Test factory with kwargs
obs4 = create_observer("MovingAverage", averaging_constant=0.05)
assert obs4.averaging_constant == 0.05

obs5 = create_observer("Histogram", bins=1024)
assert obs5.bins == 1024

# Test auto-selection (experimental)
obs6 = create_observer("auto", averaging_constant=0.01)
assert isinstance(obs6, MovingAverageMinMaxObserver)

# Test invalid observer type
try:
    create_observer("InvalidObserver")
    assert False, "Should raise ValueError"
except ValueError:
    pass
</verify>
  <done>
Observer factory creates observer instances by string name ("MinMax", "MovingAverage", "Histogram", "auto"). Auto-selection uses simple heuristics and is marked as experimental. Users can manually specify observer type for reliable calibration.
</done>
</task>

<task type="auto">
  <name>Task 4: Export advanced observers from core module</name>
  <files>src/mono_quant/core/__init__.py</files>
  <action>
Update src/mono_quant/core/__init__.py to export advanced observers:

1. Import new observers from observers:
   - from .observers import MovingAverageMinMaxObserver, HistogramObserver

2. Update __all__ list to include:
   - "MovingAverageMinMaxObserver"
   - "HistogramObserver"

3. Update module docstring to mention advanced observers and when to use them

This follows the established pattern from Phase 2 where MinMaxObserver is exported from the core module.
</action>
  <verify>
from mono_quant.core import MovingAverageMinMaxObserver, HistogramObserver

# Verify imports work
obs1 = MovingAverageMinMaxObserver(averaging_constant=0.01)
assert obs1.averaging_constant == 0.01

obs2 = HistogramObserver(bins=2048)
assert obs2.bins == 2048

# Check in __all__
from mono_quant import core
assert "MovingAverageMinMaxObserver" in core.__all__
assert "HistogramObserver" in core.__all__
</verify>
  <done>
Advanced observers exported from mono_quant.core module. Users can access MovingAverageMinMaxObserver and HistogramObserver for robust calibration in scenarios with outliers or non-uniform distributions.
</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. from mono_quant.core import MovingAverageMinMaxObserver, HistogramObserver - imports successfully
2. from mono_quant.calibration.runner import create_observer - imports successfully
3. MovingAverageMinMaxObserver smooths calibration data using EMA formula
4. HistogramObserver builds histogram and selects threshold via KL divergence
5. All observers share same interface (forward, calculate_qparams, reset)
6. Observer factory creates correct observer type by string name
7. Auto-selection (experimental) returns observer based on kwargs
8. No deprecated torch.ao.quantization APIs used
</verification>

<success_criteria>
- MovingAverageMinMaxObserver smooths out transient spikes using exponential moving average
- HistogramObserver finds optimal quantization threshold via KL divergence minimization
- Both observers follow same interface as MinMaxObserver (drop-in replacement)
- Observer factory enables easy selection by string name
- averaging_constant configurable (default 0.01 per PyTorch standard)
- Histogram bins configurable (default 2048 for good resolution)
- Auto-selection marked as experimental with warning in docstring
</success_criteria>

<output>
After completion, create `.planning/phases/03-advanced-calibration-&-int4/03-02-SUMMARY.md`
</output>
