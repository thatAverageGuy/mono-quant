---
phase: 03-advanced-calibration-&-int4
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/mono_quant/core/mappers.py
  - src/mono_quant/core/quantizers.py
  - src/mono_quant/modules/linear.py
  - src/mono_quant/core/__init__.py
  - src/mono_quant/io/formats.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can quantize a model to INT4 with group-wise scaling"
    - "Group-wise quantization handles layers smaller than group_size gracefully"
    - "INT4 quantization uses symmetric scheme by default for optimal accuracy"
    - "QuantizedLinearInt4 module stores packed INT4 weights with per-group scales"
    - "Serialization format stores INT4 metadata (group_size, scales, zero_points)"
  artifacts:
    - path: "src/mono_quant/core/mappers.py"
      provides: "Group-wise scale/zero-point calculation functions"
      min_lines: 80
      exports: ["calculate_scale_zp_groupwise"]
    - path: "src/mono_quant/core/quantizers.py"
      provides: "INT4 weight quantization with group-wise scaling"
      min_lines: 100
      exports: ["quantize_weight_int4"]
    - path: "src/mono_quant/modules/linear.py"
      provides: "QuantizedLinearInt4 module for INT4 linear layers"
      min_lines: 120
      exports: ["QuantizedLinearInt4", "quantize_linear_module_int4"]
    - path: "src/mono_quant/io/formats.py"
      provides: "Extended metadata format for INT4 parameters"
      contains: "group_size"
  key_links:
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/core/mappers.py"
      via: "quantize_weight_int4 calls calculate_scale_zp_groupwise"
      pattern: "calculate_scale_zp_groupwise"
    - from: "src/mono_quant/modules/linear.py"
      to: "src/mono_quant/core/quantizers.py"
      via: "QuantizedLinearInt4 uses quantize_weight_int4 for weight conversion"
      pattern: "quantize_weight_int4|_pack_int4_to_int8"
    - from: "src/mono_quant/io/formats.py"
      to: "src/mono_quant/core/quantizers.py"
      via: "INT4 metadata captured during quantization"
      pattern: "group_size.*int4|INT4"
---

<objective>
Build INT4 quantization support with group-wise scaling for maximum compression while maintaining acceptable accuracy. Group-wise scaling is mandatory for INT4 quantization (per-channel scaling results in unacceptable accuracy loss at 4-bit precision).

Purpose: INT4 quantization provides 2x additional compression over INT8 (4 bits vs 8 bits per weight), enabling deployment on resource-constrained hardware. Group-wise scaling with group_size=128 (industry standard from AWQ, GPTQ, HuggingFace) maintains accuracy by sharing scale/zero-point across groups of output channels.

Output: INT4 quantization with quantize_weight_int4(), QuantizedLinearInt4 module, group-wise scale/zp calculation, and safe handling of layers smaller than group_size.
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-advanced-calibration-&-int4/03-CONTEXT.md
@.planning/phases/03-advanced-calibration-&-int4/03-RESEARCH.md
@.planning/phases/02-static-quantization-&-i/02-02-SUMMARY.md
@.planning/phases/01-core-quantization-foundation/01-03-SUMMARY.md

# Context from Phase 1-2
- Existing: src/mono_quant/core/mappers.py (calculate_scale_zp_per_channel)
- Existing: src/mono_quant/core/quantizers.py (quantize_weight_int8, static_quantize)
- Existing: src/mono_quant/modules/linear.py (QuantizedLinear pattern)
- Existing: src/mono_quant/io/formats.py (QuantizationInfo, _build_metadata)
- Pattern: Local imports to avoid circular dependencies
- Pattern: Underscore prefix for internal functions
- Decision: qint4 removed from dtype range (PyTorch 2.10 doesn't support it) - use packed int8 storage

# Key Implementation Notes from RESEARCH.md
- Group size 128 is industry standard (AWQ, GPTQ, HuggingFace)
- Symmetric quantization recommended for INT4 (simpler, faster, good accuracy)
- Layers smaller than group_size: fallback to per-channel quantization (safe approach)
- Packed int8 storage: 2 INT4 values per int8 byte (bit packing)
- PyTorch doesn't have native torch.qint4 - use custom packed format
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create group-wise scale/zero-point calculation</name>
  <files>src/mono_quant/core/mappers.py</files>
  <action>
Add group-wise quantization parameter calculation to src/mono_quant/core/mappers.py:

1. calculate_scale_zp_groupwise(
     weight: torch.Tensor,
     group_size: int = 128,
     axis: int = 0,
     symmetric: bool = True,
     dtype: torch.dtype = torch.qint8,
   ) -> Tuple[torch.Tensor, torch.Tensor]

   Implementation:
   - Import math at top of file (if not already imported)
   - Validate group_size > 0
   - Get dimension size along axis: dim_size = weight.shape[axis]
   - Handle layers smaller than group_size:
     - If dim_size < group_size: fallback to calculate_scale_zp_per_channel
     - This is the safe approach per RESEARCH.md Pitfall 1
   - Calculate number of groups: num_groups = dim_size // group_size
   - Handle remainder groups: if dim_size % group_size != 0, add partial group
   - Reshape weight for group-wise processing:
     - Move axis to dim 0: weight_permuted = weight.transpose(0, axis)
     - Reshape: (num_groups, group_size, *remaining_dims)
   - Calculate per-group min/max using amin/amax along appropriate dims
   - For each group, calculate scale and zero_point
   - symmetric=True: zero_point = 0 for all groups, scale = max(abs(min), abs(max)) / qmax
   - symmetric=False: use asymmetric affine calculation
   - Return scales (shape: num_groups,), zero_points (shape: num_groups,)

2. _pack_int4_to_int8(
     int4_tensor: torch.Tensor,
   ) -> torch.Tensor

   Implementation:
   - Pack two INT4 values into one INT8 byte
   - Input: int4_tensor with values in range [-8, 7] (INT4)
   - Output: packed int8 tensor with half the elements
   - Algorithm: For i from 0 to len//2:
     - low = int4_tensor[2*i] & 0x0F
     - high = (int4_tensor[2*i+1] & 0x0F) << 4
     - packed[i] = low | high
   - This follows standard bit packing pattern from AWQ/GPTQ

Add comprehensive docstrings explaining:
- Group-wise quantization divides output channels into groups
- Each group shares a scale and zero-point
- Fallback to per-channel for layers smaller than group_size
- Packing format for INT4 storage
</action>
  <verify>
import torch
from mono_quant.core.mappers import calculate_scale_zp_groupwise

# Test with layer larger than group_size
w = torch.randn(256, 128)  # 256 output channels
scale, zp = calculate_scale_zp_groupwise(w, group_size=128, axis=0)
assert scale.shape[0] == 2  # 256 / 128 = 2 groups
assert zp.shape[0] == 2

# Test with layer smaller than group_size (fallback)
w_small = torch.randn(64, 128)  # 64 < 128
scale_small, zp_small = calculate_scale_zp_groupwise(w_small, group_size=128, axis=0)
assert scale_small.shape[0] == 64  # Falls back to per-channel

# Test symmetric vs asymmetric
w_test = torch.randn(128, 64)
scale_sym, zp_sym = calculate_scale_zp_groupwise(w_test, group_size=128, symmetric=True)
assert torch.all(zp_sym == 0)  # Symmetric has zero_point = 0
</verify>
  <done>
Group-wise scale/zero-point calculation supports INT4 quantization with configurable group_size (default 128). Handles layers smaller than group_size by falling back to per-channel quantization. Supports both symmetric and asymmetric schemes.
</done>
</task>

<task type="auto">
  <name>Task 2: Create INT4 weight quantization function</name>
  <files>src/mono_quant/core/quantizers.py</files>
  <action>
Add INT4 weight quantization to src/mono_quant/core/quantizers.py:

1. quantize_weight_int4(
     weight: torch.Tensor,
     group_size: int = 128,
     symmetric: bool = True,
     axis: int = 0,
   ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]

   Implementation:
   - Import calculate_scale_zp_groupwise from mono_quant.core.mappers (local import)
   - Import _pack_int4_to_int8 from mono_quant.core.mappers (local import)
   - Validate group_size > 0
   - Get dimension size: dim_size = weight.shape[axis]
   - Handle fallback: if dim_size < group_size:
     - Use quantize_weight_int8 as fallback (return qint8, not int4)
     - Add warning logging about fallback
     - Return with scales/zp from per-channel calculation
   - Calculate group-wise scale and zero-point:
     - scales, zero_points = calculate_scale_zp_groupwise(weight, group_size, axis, symmetric)
   - Reshape weight for group-wise processing (same as calculate_scale_zp_groupwise)
   - For each group:
     - Extract group weights
     - Quantize to INT4 range [-8, 7]
     - symmetric: int4_values = torch.clamp(weights / scale).round().to(torch.int32) - 8
     - asymmetric: standard affine formula
   - Concatenate quantized groups
   - Pack INT4 values to INT8 using _pack_int4_to_int8
   - Return (packed_int8_weights, scales, zero_points)

2. Add INT4 dtype handling

   - Note: PyTorch doesn't have torch.qint4 (per Phase 1 decision)
   - Use packed int8 storage with metadata indicating INT4
   - Caller (QuantizedLinearInt4) will handle dequantization

3. Update module docstring to mention INT4 support

Add comprehensive docstring explaining:
- INT4 range is [-8, 7] (signed 4-bit integer)
- Packed storage format (2 INT4 values per INT8 byte)
- Group-wise scaling required for INT4 accuracy
- Fallback behavior for small layers
</action>
  <verify>
import torch
from mono_quant.core.quantizers import quantize_weight_int4

# Test basic INT4 quantization
w = torch.randn(256, 128)
packed, scales, zp = quantize_weight_int4(w, group_size=128, symmetric=True)
assert packed.dtype == torch.int8  # Packed in int8 storage
assert packed.numel() == w.numel() // 2  # Half the size (4-bit packing)
assert scales.shape[0] == 2  # 256 / 128 = 2 groups

# Test symmetric zero_point
_, scales_sym, zp_sym = quantize_weight_int4(w, group_size=128, symmetric=True)
assert torch.all(zp_sym == 0)

# Test layer smaller than group_size (fallback)
w_small = torch.randn(64, 128)
packed_small, scales_small, zp_small = quantize_weight_int4(w_small, group_size=128)
# Should fall back gracefully without error
</verify>
  <done>
INT4 weight quantization with group-wise scaling. Uses packed int8 storage (2 INT4 values per byte). Falls back to per-channel quantization for layers smaller than group_size. Symmetric quantization by default for optimal accuracy.
</done>
</task>

<task type="auto">
  <name>Task 3: Create QuantizedLinearInt4 module</name>
  <files>src/mono_quant/modules/linear.py</files>
  <action>
Add QuantizedLinearInt4 module to src/mono_quant/modules/linear.py:

1. QuantizedLinearInt4(nn.Module) class

   Attributes:
   - in_features: int (input dimension)
   - out_features: int (output dimension)
   - group_size: int (number of channels per group)
   - packed_weight: torch.Tensor (INT4 weights packed in int8 storage)
   - scales: torch.Tensor (per-group scale factors, shape: num_groups,)
   - zero_points: torch.Tensor (per-group zero-points, shape: num_groups,)
   - bias: Optional[torch.Tensor] (bias term, preserved but not quantized)

   Methods:

   a. __init__(
        self,
        in_features: int,
        out_features: int,
        packed_weight: torch.Tensor,
        scales: torch.Tensor,
        zero_points: torch.Tensor,
        group_size: int = 128,
        bias: Optional[torch.Tensor] = None,
      )
      - Store all parameters
      - Register packed_weight, scales, zero_points as buffers (not parameters)
      - Register bias as parameter if provided

   b. forward(self, x: torch.Tensor) -> torch.Tensor
      - Unpack INT4 weights to float32 using scales and zero_points
      - For each group:
        - Extract packed INT4 values for group
        - Unpack: low_4bit = packed & 0x0F, high_4bit = (packed >> 4) & 0x0F
        - Convert to signed: if value >= 8, subtract 16 (two's complement)
        - Dequantize: dequantized = (int4_value - zero_point) * scale
      - Reconstruct full weight tensor from dequantized groups
      - Compute: F.linear(x, weight_dequantized, self.bias)
      - Return result

   c. @classmethod from_float(
        cls,
        module: nn.Linear,
        group_size: int = 128,
        symmetric: bool = True,
      ) -> "QuantizedLinearInt4"
      - Local import quantize_weight_int4 from mono_quant.core.quantizers
      - Get weight from module.weight.data
      - Call quantize_weight_int4(weight, group_size, symmetric, axis=0)
      - Create new QuantizedLinearInt4 instance
      - Copy bias if present
      - Return new instance

2. quantize_linear_module_int4(
     module: nn.Linear,
     group_size: int = 128,
     symmetric: bool = True,
   ) -> QuantizedLinearInt4
   - Helper function to convert nn.Linear to QuantizedLinearInt4
   - Calls QuantizedLinearInt4.from_float()
   - Returns quantized module

3. Export new classes/functions

   - Update __all__ in modules/linear.py to include:
     - "QuantizedLinearInt4"
     - "quantize_linear_module_int4"

Add comprehensive docstrings explaining:
- INT4 dequantization happens during forward pass
- Unpacking follows bit packing format from mappers.py
- Group-wise dequantization: each group uses its scale/zero_point
</action>
  <verify>
import torch
import torch.nn as nn
from mono_quant.modules.linear import quantize_linear_module_int4, QuantizedLinearInt4

# Test basic conversion
linear = nn.Linear(128, 256)
q_linear = quantize_linear_module_int4(linear, group_size=128)
assert isinstance(q_linear, QuantizedLinearInt4)
assert q_linear.in_features == 128
assert q_linear.out_features == 256

# Test forward pass
x = torch.randn(32, 128)
output = q_linear(x)
assert output.shape == (32, 256)

# Test from_float classmethod
q_linear2 = QuantizedLinearInt4.from_float(linear, group_size=128)
assert isinstance(q_linear2, QuantizedLinearInt4)

# Verify group size
assert q_linear.group_size == 128
assert q_linear.scales.shape[0] == 2  # 256 / 128
</verify>
  <done>
QuantizedLinearInt4 module supports INT4 quantized linear layers with group-wise scaling. Dequantizes weights during forward pass using per-group scale and zero-point. Handles unpacking from packed int8 storage format.
</done>
</task>

<task type="auto">
  <name>Task 4: Extend serialization for INT4 metadata</name>
  <files>src/mono_quant/io/formats.py</files>
  <action>
Update src/mono_quant/io/formats.py to support INT4 metadata:

1. Extend QuantizationInfo dataclass

   Add new optional fields:
   - group_size: Optional[int] = None (group size for INT4 quantization)
   - bits: Optional[int] = None (bits per weight: 4 or 8)

2. Update _build_metadata function

   Add INT4-specific metadata when QuantizationInfo has group_size:
   - "group_size": str(info.group_size) if info.group_size else None
   - "bits": str(info.bits) if info.bits else None
   - Only include if values are not None

3. Update save_model metadata documentation

   Add INT4 parameters to metadata description:
   - For INT4: includes group_size, per-group scales/zero_points
   - Format: scales and zero_points stored as separate tensors in state_dict

4. Update load_model to handle INT4 metadata

   When loading INT4 models:
   - Read group_size from metadata if present
   - Return metadata dict with INT4 parameters for caller to use

Note: This task focuses on metadata only. The actual INT4 model loading will be handled by the QuantizedLinearInt4 module during state_dict loading.

Add docstring updates explaining INT4 metadata structure.
</action>
  <verify>
from mono_quant.io.formats import QuantizationInfo, _build_metadata

# Test QuantizationInfo with INT4 fields
info = QuantizationInfo(
    selected_layers=["0"],
    skipped_layers=[],
    calibration_samples_used=100,
    dtype=torch.int32,  # Using int32 for packed storage
    symmetric=True,
    group_size=128,
    bits=4,
)

metadata = _build_metadata(info)
assert "group_size" in metadata
assert metadata["group_size"] == "128"
assert "bits" in metadata
assert metadata["bits"] == "4"
</verify>
  <done>
Serialization format extended to capture INT4-specific metadata (group_size, bits). Metadata stored as string values per safetensors requirement. INT4 models can be saved with full quantization parameters for reproducibility.
</done>
</task>

<task type="auto">
  <name>Task 5: Export INT4 functions from core module</name>
  <files>src/mono_quant/core/__init__.py</files>
  <action>
Update src/mono_quant/core/__init__.py to export INT4 functions:

1. Import new functions from mappers:
   - from .mappers import calculate_scale_zp_groupwise

2. Import new functions from quantizers:
   - from .quantizers import quantize_weight_int4

3. Update __all__ list to include:
   - "calculate_scale_zp_groupwise"
   - "quantize_weight_int4"

4. Update module docstring to mention INT4 support

This follows the established pattern from Phase 1-2 where core quantization functions are exported from the core module.
</action>
  <verify>
from mono_quant.core import calculate_scale_zp_groupwise, quantize_weight_int4
import torch

# Verify imports work
w = torch.randn(256, 128)
scale, zp = calculate_scale_zp_groupwise(w, group_size=128)
packed, scales, zp = quantize_weight_int4(w, group_size=128)

# Check in __all__
from mono_quant import core
assert "calculate_scale_zp_groupwise" in core.__all__
assert "quantize_weight_int4" in core.__all__
</verify>
  <done>
INT4 quantization functions exported from mono_quant.core module. Users can access group-wise scale/zp calculation and INT4 weight quantization directly.
</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. from mono_quant.core import quantize_weight_int4, calculate_scale_zp_groupwise - imports successfully
2. from mono_quant.modules.linear import QuantizedLinearInt4 - imports successfully
3. INT4 weight quantization produces packed int8 storage with half the element count
4. Group-wise scales and zero-points calculated correctly (num_groups = dim_size // group_size)
5. Layers smaller than group_size fall back to per-channel without error
6. QuantizedLinearInt4 forward pass produces correctly shaped outputs
7. INT4 metadata (group_size, bits) captured in serialization format
8. Symmetric INT4 quantization has zero_point = 0 for all groups
</verification>

<success_criteria>
- User can quantize weights to INT4 using quantize_weight_int4() with configurable group_size
- Group-wise scaling calculates per-group scale and zero-point (default 128 channels per group)
- Layers smaller than group_size fall back to per-channel quantization (safe handling per RESEARCH.md)
- QuantizedLinearInt4 module dequantizes during forward pass using per-group parameters
- INT4 weights stored in packed int8 format (2x compression over INT8)
- Serialization captures INT4 metadata (group_size, bits) for reproducibility
- Symmetric quantization by default (zero_point = 0) for optimal INT4 accuracy
</success_criteria>

<output>
After completion, create `.planning/phases/03-advanced-calibration-&-int4/03-01-SUMMARY.md`
</output>
