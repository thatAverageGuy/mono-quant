---
phase: 02-static-quantization-&-i
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/mono_quant/io/formats.py
  - src/mono_quant/io/__init__.py
  - pyproject.toml
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can save quantized model to PyTorch format (.pt/.pth)"
    - "User can save quantized model to Safetensors format (.safetensors)"
    - "User can load a quantized model from disk"
    - "Safetensors metadata stores quantization parameters, model info, versions, metrics"
    - "Metadata values are strings only (safetensors constraint)"
  artifacts:
    - path: "src/mono_quant/io/formats.py"
      provides: "Save/load functions for PyTorch and Safetensors formats"
      min_lines: 200
      exports: ["save_pytorch", "load_pytorch", "save_safetensors", "load_safetensors", "save_model", "load_model"]
    - path: "pyproject.toml"
      provides: "safetensors dependency added"
      contains: "safetensors>=0.4"
  key_links:
    - from: "src/mono_quant/io/formats.py"
      to: "safetensors.torch"
      via: "save_file and safe_open for serialization"
      pattern: "from safetensors\.torch import"
    - from: "src/mono_quant/io/formats.py"
      to: "torch.save"
      via: "PyTorch format fallback"
      pattern: "torch\.save\(.*state_dict"
    - from: "save_model"
      to: "QuantizationInfo"
      via: "Metadata extraction from quantization result"
      pattern: "quantization_info\.|info\."
---

<objective>
Implement model serialization for quantized models supporting both PyTorch (.pt/.pth) and Safetensors formats with comprehensive metadata.

Purpose: Quantized models need persistent storage for deployment. Safetensors provides secure serialization (no pickle), while PyTorch format offers compatibility. Metadata preserves quantization parameters, model info, and metrics for reproducibility.

Output: save_model() and load_model() functions supporting .pt/.pth and .safetensors formats with automatic metadata handling.
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-static-quantization-&-i/02-CONTEXT.md
@.planning/phases/02-static-quantization-&-i/02-RESEARCH.md
@.planning/phases/02-static-quantization-&-i/02-01-SUMMARY.md

# Context from Phase 1
- Existing: src/mono_quant/io/handlers.py (input handling patterns)
- Existing: src/mono_quant/io/__init__.py (existing I/O exports)

# Context from RESEARCH.md
- Safetensors metadata values MUST be strings (JSON-serialize complex types)
- CRITICAL: __metadata__ is the special key for custom metadata
- Pattern: save_file(tensors, path, metadata=string_metadata)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add safetensors dependency to project</name>
  <files>pyproject.toml</files>
  <action>
Update pyproject.toml:

1. Add safetensors to dependencies list:
   dependencies = [
       "torch>=2.0.0",
       "safetensors>=0.4",
   ]

2. Add tqdm to dev dependencies (optional for progress bars):
   dev = [
       "pytest>=7.0.0",
       "ruff>=0.1.0",
       "mypy>=1.0.0",
       "tqdm>=4.66",
   ]

Commit with message: "feat(02-03): add safetensors and tqdm dependencies"
</action>
  <verify>
grep -q "safetensors>=0.4" pyproject.toml
grep -q "tqdm>=4.66" pyproject.toml
pip install -e . --quiet  # Should install without errors
python -c "import safetensors; print(safetensors.__version__)"
</verify>
  <done>
safetensors>=0.4 added to runtime dependencies, tqdm>=4.66 added to dev dependencies for progress bar support.
</done>
</task>

<task type="auto">
  <name>Task 2: Create Safetensors format handlers</name>
  <files>src/mono_quant/io/formats.py</files>
  <action>
Create src/mono_quant/io/formats.py with Safetensors support:

1. save_safetensors(
     model: Union[nn.Module, Dict[str, torch.Tensor]],
     path: str,
     metadata: Optional[Dict[str, Any]] = None
   ) -> None

   Implementation:
   - Imports: json, torch, torch.nn, safetensors.torch (save_file, safe_open)
   - Extract state_dict: model.state_dict() if nn.Module else model
   - If metadata is None: create default metadata dict
   - Convert all metadata values to strings:
     - For each key, value in metadata.items():
       - If isinstance(value, str): use as-is
       - Else: json.dumps(value) for complex types
   - Call save_file(state_dict, path, metadata=string_metadata)
   - Add docstring explaining metadata string requirement

2. load_safetensors(
     path: str,
     device: torch.device = torch.device("cpu")
   ) -> Dict[str, torch.Tensor]

   Implementation:
   - Initialize empty tensors dict
   - Use safe_open context manager:
     - with safe_open(path, framework="pt", device=str(device)) as f:
       - for key in f.keys():
         - tensors[key] = f.get_tensor(key)
   - Return tensors dict
   - Add docstring explaining zero-copy loading

3. _build_metadata(
     quantization_info: Optional[QuantizationInfo] = None,
     original_size_mb: Optional[float] = None,
     quantized_size_mb: Optional[float] = None,
     compression_ratio: Optional[float] = None,
     sqnr_db: Optional[float] = None,
   ) -> Dict[str, str]

   Implementation:
   - Initialize empty metadata dict
   - If quantization_info:
     - Add "quantization_dtype": str(quantization_info.dtype)
     - Add "scheme": "symmetric" if quantization_info.symmetric else "asymmetric"
     - Add "per_channel": "true" if quantization_info.per_channel else "false"
     - Add "selected_layers": json.dumps(quantization_info.selected_layers)
     - Add "calibration_samples": str(quantization_info.calibration_samples_used)
   - Add version info:
     - "mono_quant_version": get version from importlib.metadata
     - "pytorch_version": torch.__version__
   - If size metrics provided:
     - "original_size_mb": str(original_size_mb)
     - "quantized_size_mb": str(quantized_size_mb)
     - "compression_ratio": str(compression_ratio)
   - If sqnr_db provided: "sqnr_db": str(sqnr_db)
   - Add "format": "safetensors"
   - Return metadata (all string values)

Add module docstring explaining Safetensors format advantages and metadata constraints.
</action>
  <verify>
from mono_quant.io.formats import save_safetensors, load_safetensors
import torch.nn as nn

model = nn.Linear(10, 20)
save_safetensors(model, "test_model.safetensors", metadata={"test": "value"})
loaded = load_safetensors("test_model.safetensors")
assert "weight" in loaded
import os; os.remove("test_model.safetensors")
</verify>
  <done>
Safetensors save/load functions with string-only metadata conversion. _build_metadata creates comprehensive quantization metadata.
</done>
</task>

<task type="auto">
  <name>Task 3: Create PyTorch format handlers and unified API</name>
  <files>src/mono_quant/io/formats.py</files>
  <action>
Add to src/mono_quant/io/formats.py:

1. save_pytorch(
     model: Union[nn.Module, Dict[str, torch.Tensor]],
     path: str
   ) -> None

   Implementation:
   - Extract state_dict: model.state_dict() if nn.Module else model
   - torch.save(state_dict, path)
   - Add docstring noting pickle security considerations

2. load_pytorch(
     path: str,
     device: torch.device = torch.device("cpu")
   ) -> Dict[str, torch.Tensor]

   Implementation:
   - return torch.load(path, map_location=device)
   - Add docstring with security warning about pickle

3. save_model(
     model: Union[nn.Module, Dict[str, torch.Tensor]],
     path: str,
     quantization_info: Optional[QuantizationInfo] = None,
     original_size_mb: Optional[float] = None,
     quantized_size_mb: Optional[float] = None,
     compression_ratio: Optional[float] = None,
     sqnr_db: Optional[float] = None,
   ) -> None

   Implementation:
   - Detect format from path extension:
     - .safetensors: use save_safetensors
     - .pt, .pth: use save_pytorch (metadata not supported in standard torch.save)
   - Build metadata via _build_metadata with all provided parameters
   - Call appropriate save function
   - Add docstring explaining format auto-detection

4. load_model(
     path: str,
     device: torch.device = torch.device("cpu")
   ) -> Dict[str, torch.Tensor]

   Implementation:
   - Detect format from path extension
   - .safetensors: use load_safetensors
   - .pt, .pth: use load_pytorch
   - Return loaded state_dict
   - Add docstring explaining format auto-detection

Update src/mono_quant/io/__init__.py to export:
- save_model, load_model
- save_pytorch, load_pytorch
- save_safetensors, load_safetensors
</action>
  <verify>
from mono_quant.io import save_model, load_model
import torch.nn as nn

model = nn.Linear(10, 20)
# Test PyTorch format
save_model(model, "test.pt")
loaded = load_model("test.pt")
assert "weight" in loaded
# Test Safetensors format
save_model(model, "test.safetensors")
loaded = load_model("test.safetensors")
assert "weight" in loaded
import os; os.remove("test.pt"); os.remove("test.safetensors")
</verify>
  <done>
Unified save_model/load_model API auto-detects format from extension (.pt/.pth for PyTorch, .safetensors for Safetensors). Metadata preserved in Safetensors format.
</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. from mono_quant.io import save_model, load_model - imports successfully
2. save_model(model, "output.pt") saves to PyTorch format
3. save_model(model, "output.safetensors") saves to Safetensors format with metadata
4. load_model(path) loads from either format
5. Metadata includes quantization parameters, model info, versions, metrics
6. All metadata values are strings (safetensors constraint satisfied)
7. safetensors>=0.4 in pyproject.toml dependencies
</verification>

<success_criteria>
- save_model supports .pt/.pth (PyTorch) and .safetensors formats
- load_model auto-detects format from file extension
- Safetensors metadata stores quantization dtype, scheme, layer info, versions, metrics
- Metadata values are strings only (complex types JSON-serialized)
- safetensors dependency added to project
</success_criteria>

<output>
After completion, create `.planning/phases/02-static-quantization-&-i/02-03-SUMMARY.md`
</output>
