---
phase: 02-static-quantization-&-i
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/mono_quant/core/observers.py
  - src/mono_quant/core/quantizers.py
  - src/mono_quant/core/__init__.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can specify which layer types to quantize (layer_types parameter)"
    - "User can exclude specific layer types from quantization (skip_types parameter)"
    - "User can provide exact layer names for selective quantization (layer_names parameter)"
    - "static_quantize function applies calibration before quantization"
    - "Skipped layers are reported back to user"
    - "User can dequantize model back to FP32 (dequantize_model function)"
  artifacts:
    - path: "src/mono_quant/core/observers.py"
      provides: "Layer selection functions for type-based and name-based filtering"
      min_lines: 100
      exports: ["_select_layers_by_type", "_select_layers_by_name"]
    - path: "src/mono_quant/core/quantizers.py"
      provides: "static_quantize function with calibration and layer selection, plus dequantize_model"
      min_lines: 180
      exports: ["static_quantize", "dequantize_model"]
  key_links:
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/core/observers.py"
      via: "Import of layer selection and observer functions"
      pattern: "from mono_quant\.core\.observers import"
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/calibration/runner.py"
      via: "run_calibration call for activation range observation"
      pattern: "run_calibration\(.*model.*calibration_data"
    - from: "static_quantize"
      to: "_quantize_int8_model"
      via: "Reusing Phase 1 dynamic quantization for layer replacement"
      pattern: "_quantize_int8_model\(.*model"
    - from: "dequantize_model"
      to: "dequantize_weight"
      via: "Reusing Phase 1 weight dequantization for model-level conversion"
      pattern: "dequantize_weight\(.*parameter"
---

<objective>
Implement layer selection API, static quantization function that applies calibration before quantizing selected layers, and model-level dequantization to convert quantized models back to FP32.

Purpose: Static quantization requires calibration data to determine activation ranges, and layer selection to control which layers are quantized. This plan provides type-based filtering (include/exclude), name-based selection, the static_quantize entry point that orchestrates calibration + quantization, and dequantize_model for converting quantized models back to float32.

Output: static_quantize() function with layer_types, skip_types, layer_names, and calibration_data parameters, plus dequantize_model() for FP32 conversion.
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-static-quantization-&-i/02-CONTEXT.md
@.planning/phases/02-static-quantization-&-i/02-RESEARCH.md
@.planning/phases/02-static-quantization-&-i/02-01-SUMMARY.md

# Context from Phase 1
- Existing: src/mono_quant/core/quantizers.py (_quantize_int8_model, _quantize_fp16_model, dequantize_weight)
- Existing: src/mono_quant/io/handlers.py (_prepare_model for copying)
- Pattern: Local imports to avoid circular dependencies
- Pattern: Config priority: kwargs > config > defaults
- Pattern: Underscore prefix for internal functions
- dequantize_weight handles both INT8 (torch.dequantize) and FP16 (to float32) tensors

# Context from 02-01
- Existing: MinMaxObserver for tracking activation ranges
- Existing: run_calibration for forward pass execution
- Existing: _normalize_calibration_data for DataLoader support
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create layer selection functions</name>
  <files>src/mono_quant/core/observers.py</files>
  <action>
Add layer selection functions to src/mono_quant/core/observers.py:

1. _select_layers_by_type(
     model: nn.Module,
     layer_types: LayerTypes,
     skip_types: Optional[LayerTypes] = None
   ) -> Tuple[List[str], List[str]]

   Implementation:
   - Import nn, typing (List, Tuple, Type, Union, Optional)
   - Define LayerTypes = Union[Type[nn.Module], Tuple[Type[nn.Module], ...]]
   - Normalize layer_types to tuple if not already tuple
   - Normalize skip_types to tuple if provided, else ()
   - Initialize selected=[], skipped=[]
   - Iterate model.named_modules():
     - Skip "" (root module)
     - If isinstance(module, skip_types): append name to skipped
     - Elif isinstance(module, layer_types): append name to selected
     - Else: append name to skipped
   - Return (selected, skipped)

2. _select_layers_by_name(
     model: nn.Module,
     layer_names: Optional[List[str]] = None,
     skip_names: Optional[List[str]] = None
   ) -> Tuple[List[str], List[str]]

   Implementation:
   - Initialize selected=set(), skip_set=set(skip_names or [])
   - Iterate model.named_modules():
     - Skip "" (root module)
     - If layer_names provided and name in layer_names: add to selected if not in skip_set
     - Return sorted(list(selected)), list(skip_set)

3. _merge_selection_results(
     *selection_results: Tuple[List[str], List[str]]
   ) -> Tuple[List[str], List[str]]

   Implementation:
   - Union all selected lists, remove duplicates
   - Union all skipped lists, remove any that are in selected
   - Return (sorted(selected), sorted(skipped))

Add docstrings explaining:
- layer_types uses isinstance for type matching
- skip_types excludes specific module types
- layer_names provides exact name matching
- Results are merged when combining type and name selection
</action>
  <verify>
from mono_quant.core.observers import _select_layers_by_type, _select_layers_by_name
import torch.nn as nn

model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
selected, skipped = _select_layers_by_type(model, nn.Linear)
assert "0" in selected and "2" in selected  # Linear layers
assert "1" in skipped  # ReLU skipped
</verify>
  <done>
Layer selection functions support type-based filtering (include/exclude), name-based selection, and merging of multiple selection criteria.
</done>
</task>

<task type="auto">
  <name>Task 2: Create static_quantize function</name>
  <files>src/mono_quant/core/quantizers.py</files>
  <action>
Add static_quantize function to src/mono_quant/core/quantizers.py:

1. static_quantize(
     model: Union[nn.Module, Dict[str, torch.Tensor]],
     calibration_data: CalibrationData,
     dtype: torch.dtype = torch.qint8,
     symmetric: bool = False,
     per_channel: bool = True,
     layer_types: Optional[LayerTypes] = None,
     skip_types: Optional[LayerTypes] = None,
     layer_names: Optional[List[str]] = None,
     skip_names: Optional[List[str]] = None,
     num_calibration_samples: int = 150,
     config: Optional["QuantizationConfig"] = None,
   ) -> Tuple[nn.Module, QuantizationInfo]

   Implementation:
   - Handle config override (config priority pattern)
   - Define CalibrationData type alias or import from calibration
   - Local imports: _prepare_model from io.handlers, run_calibration from calibration.runner
   - Local imports: _select_layers_by_type, _select_layers_by_name from core.observers
   - Local imports: quantize_weight_int8, _quantize_int8_model from same module

   - Copy model via _prepare_model (preserves original)

   - If dtype == torch.float16:
     - Return _quantize_fp16_model(model_copy) with QuantizationInfo

   - For INT8 static quantization:
     a) Determine layers to quantize:
        - If layer_types or skip_types: use _select_layers_by_type
        - If layer_names or skip_names: use _select_layers_by_name
        - Merge results, get final selected_layers list

     b) Run calibration:
        - Attach MinMaxObserver to selected layers (store in dict)
        - Call run_calibration(model_copy, calibration_data, num_calibration_samples)
        - Collect scale/zero_point from observers

     c) Quantize selected layers:
        - Reuse _quantize_int8_model logic but only for selected layers
        - Apply quantization with calibration-derived scales
        - Track skipped layers

   - Return (model_copy, QuantizationInfo)

2. Create QuantizationInfo dataclass:
   - selected_layers: List[str]
   - skipped_layers: List[str]
   - calibration_samples_used: int
   - dtype: torch.dtype
   - symmetric: bool

Add docstring explaining:
- Calibration data determines activation ranges for static quantization
- Layer selection controls which layers are quantized
- Returns copy of model (original unchanged)
</action>
  <verify>
from mono_quant.core.quantizers import static_quantize
import torch.nn as nn

model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
calibration_data = [torch.randn(4, 10) for _ in range(50)]
q_model, info = static_quantize(model, calibration_data, layer_types=[nn.Linear])
assert id(q_model) != id(model)  # Different objects (original preserved)
assert len(info.selected_layers) >= 1  # At least one Linear quantized
assert info.calibration_samples_used == 50  # Limited by data size
</verify>
  <done>
static_quantize applies calibration before quantization, supports layer type/name selection, returns quantized model copy with QuantizationInfo metadata.
</done>
</task>

<task type="auto">
  <name>Task 3: Export static_quantize from core module</name>
  <files>src/mono_quant/core/__init__.py</files>
  <action>
Update src/mono_quant/core/__init__.py to export:
- MinMaxObserver (from .observers)
- _select_layers_by_type (from .observers)
- static_quantize (from .quantizers)
- QuantizationInfo (from .quantizers)
- dequantize_model (from .quantizers) - added in Task 4

Also export existing:
- dynamic_quantize
- quantize_weight_int8
- quantize_weight_fp16
- dequantize_weight
</action>
  <verify>
from mono_quant.core import static_quantize, MinMaxObserver, dynamic_quantize, dequantize_model
assert callable(static_quantize)
assert MinMaxObserver is not None
assert callable(dequantize_model)
</verify>
  <done>
Core module exports static_quantize, dequantize_model, and related functions for public API use.
</done>
</task>

<task type="auto">
  <name>Task 4: Create dequantize_model function</name>
  <files>src/mono_quant/core/quantizers.py</files>
  <action>
Add dequantize_model function to src/mono_quant/core/quantizers.py:

1. dequantize_model(
     model: nn.Module,
     inplace: bool = False
   ) -> nn.Module

   Implementation:
   - If not inplace: create copy via _prepare_model (local import from io.handlers)
   - Iterate over all named_parameters() in the model:
     - For each parameter, reuse dequantize_weight from same module
     - dequantize_weight handles:
       a) INT8 quantized tensors: calls torch.dequantize()
       b) FP16 tensors: casts to torch.float32
       c) FP32 tensors: returns as-is
     - Replace parameter data with dequantized version
   - For buffers (non-trainable tensors like batch norm stats):
     - Convert qint8/float16 to float32 using .to(torch.float32)
   - Return the dequantized model (copy or original if inplace)

Add docstring explaining:
- Converts quantized model back to full FP32 precision
- Reuses Phase 1's dequantize_weight for individual tensors
- inplace=False preserves original (default for consistency with static_quantize)
- Use case: comparison testing, debugging, recovery from quantization

Note: This is a model-level wrapper around the existing dequantize_weight function from Phase 1, following the established pattern of reusing lower-level utilities.
</action>
  <verify>
from mono_quant.core.quantizers import static_quantize, dequantize_model
import torch.nn as nn

model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
calibration_data = [torch.randn(4, 10) for _ in range(50)]
q_model, info = static_quantize(model, calibration_data, layer_types=[nn.Linear])

# Dequantize back to FP32
dq_model = dequantize_model(q_model)
assert id(dq_model) != id(q_model)  # Different objects (inplace=False default)
for name, param in dq_model.named_parameters():
    assert param.dtype == torch.float32, f"{name} has dtype {param.dtype}, expected float32"
</verify>
  <done>
dequantize_model converts quantized models back to FP32 using dequantize_weight from Phase 1. Supports inplace=False default for consistency with static_quantize API.
</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. from mono_quant.core import static_quantize - imports successfully
2. static_quantize accepts calibration_data (List[torch.Tensor] or DataLoader)
3. layer_types parameter filters which layers to quantize
4. skip_types parameter excludes specific layer types
5. layer_names parameter selects exact layers by name
6. Original model is preserved (returns different object)
7. QuantizationInfo reports selected/skipped layers and calibration samples used
8. from mono_quant.core import dequantize_model - imports successfully
9. dequantize_model converts quantized model back to FP32
10. dequantize_model preserves original when inplace=False (default)
</verification>

<success_criteria>
- static_quantize applies calibration before quantization
- Layer selection works by type (include/exclude) and by exact name
- Calibration data is normalized from DataLoader or tensor list
- Original model is preserved (copy returned)
- QuantizationInfo provides metadata about what was quantized
- dequantize_model converts quantized models back to FP32
- dequantize_model reuses Phase 1's dequantize_weight function
</success_criteria>

<output>
After completion, create `.planning/phases/02-static-quantization-&-i/02-02-SUMMARY.md`
</output>
