---
phase: 02-static-quantization-&-i
plan: 04
type: execute
wave: 3
depends_on: ["02-02", "02-03"]
files_modified:
  - src/mono_quant/io/validation.py
  - src/mono_quant/io/__init__.py
  - src/mono_quant/__init__.py
  - src/mono_quant/core/quantizers.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can see SQNR metrics after quantization"
    - "User can see model size comparison (original vs quantized, compression ratio)"
    - "System validates quantized model can be loaded from disk"
    - "System validates quantized weights are in valid range"
    - "Validation behavior is configurable (on_failure='error'|'warn'|'ignore')"
    - "static_quantize is exported from public API"
  artifacts:
    - path: "src/mono_quant/io/validation.py"
      provides: "Validation metrics and checks for quantized models"
      min_lines: 200
      exports: ["ValidationResult", "calculate_model_size", "calculate_sqnr", "validate_quantization"]
    - path: "src/mono_quant/__init__.py"
      provides: "Public API exports including static_quantize"
      exports: ["static_quantize", "dynamic_quantize", "QuantizationConfig"]
  key_links:
    - from: "src/mono_quant/io/validation.py"
      to: "src/mono_quant/io/formats.py"
      via: "save_model/load_model for load-and-run test"
      pattern: "save_model|load_model"
    - from: "src/mono_quant/core/quantizers.py"
      to: "src/mono_quant/io/validation.py"
      via: "validate_quantization called after static_quantize"
      pattern: "validate_quantization\(.*original.*quantized"
    - from: "src/mono_quant/__init__.py"
      to: "src/mono_quant/core"
      via: "Export static_quantize to public API"
      pattern: "from mono_quant\.core import.*static_quantize"
---

<objective>
Implement validation metrics (SQNR, model size, load test, weight range check) and integrate with static_quantize for automatic validation. Export complete public API.

Purpose: Quantization quality assurance requires measurable metrics. SQNR indicates quantization accuracy, model size shows compression benefit, load test verifies serialized models work, and weight range check catches numerical errors. Configurable failure behavior provides flexibility.

Output: validate_quantization() function, ValidationResult dataclass, and static_quantize integrated with validation.
</objective>

<execution_context>
@C:\Users\ghost\.thatAverageGuy\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.thatAverageGuy\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-static-quantization-&-i/02-CONTEXT.md
@.planning/phases/02-static-quantization-&-i/02-RESEARCH.md
@.planning/phases/02-static-quantization-&-i/02-02-SUMMARY.md
@.planning/phases/02-static-quantization-&-i/02-03-SUMMARY.md

# Context from Phase 1
- Existing: src/mono_quant/__init__.py (currently exports dynamic_quantize, QuantizationConfig)
- Existing: src/mono_quant/io/__init__.py (handler exports)

# Context from 02-02
- Existing: static_quantize in src/mono_quant/core/quantizers.py
- Existing: QuantizationInfo dataclass

# Context from 02-03
- Existing: save_model, load_model in src/mono_quant/io/formats.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation metrics module</name>
  <files>src/mono_quant/io/validation.py</files>
  <action>
Create src/mono_quant/io/validation.py with:

1. ValidationResult dataclass:
   @dataclass
   class ValidationResult:
       sqnr_db: Optional[float] = None
       original_size_mb: Optional[float] = None
       quantized_size_mb: Optional[float] = None
       compression_ratio: Optional[float] = None
       load_test_passed: bool = False
       weight_range_valid: bool = False
       errors: List[str] = field(default_factory=list)

2. calculate_model_size(model: nn.Module) -> Tuple[float, int]:
   - Initialize param_count=0, size_bytes=0
   - Loop model.parameters(): param_count += p.numel(), size_bytes += p.numel() * p.element_size()
   - Loop model.buffers(): size_bytes += b.numel() * b.element_size()
   - size_mb = size_bytes / (1024 * 1024)
   - Return (size_mb, param_count)

3. calculate_sqnr(
     original: torch.Tensor,
     quantized: torch.Tensor
   ) -> float:
   - If hasattr(quantized, 'dequantize'): quantized_fp32 = quantized.dequantize()
   - Else: quantized_fp32 = quantized.to(torch.float32)
   - noise = original - quantized_fp32
   - signal_power = (original ** 2).mean()
   - noise_power = (noise ** 2).mean()
   - If noise_power < 1e-10: return float('inf')
   - sqnr_db = 10 * torch.log10(signal_power / noise_power)
   - Return sqnr_db.item()

4. _calculate_model_sqnr(
     original: nn.Module,
     quantized: nn.Module
   ) -> float:
   - Initialize sqnr_values = []
   - For (name1, p1), (name2, p2) in zip(original.named_parameters(), quantized.named_parameters()):
     - Skip if names don't match
     - If p1.dtype == torch.float32 and p2.dtype in [torch.qint8, torch.float16]:
       - sqnr = calculate_sqnr(p1.data, p2)
       - sqnr_values.append(sqnr)
   - Return mean(sqnr_values) if sqnr_values else 0.0

5. _check_weight_ranges(model: nn.Module) -> bool:
   - For each param in model.parameters():
     - If param.dtype == torch.qint8:
       - Check dequantized values are reasonable (-10 to 10 range)
     - Elif param.dtype == torch.float16:
       - Check for finite values (not nan/inf)
   - Return True if all checks pass

6. _test_load_run(quantized: nn.Module) -> bool:
   - Local import: save_model, load_model from .formats
   - Import tempfile
   - with tempfile.NamedTemporaryFile(suffix=".safetensors", delete=False) as f:
     - temp_path = f.name
   - Try: save_model(quantized.state_dict(), temp_path)
   - Try: loaded = load_model(temp_path)
   - Try: quantized.load_state_dict(loaded)
   - Create test_input with correct shape (use first layer in_features)
   - Run forward pass: _ = quantized(test_input)
   - Cleanup: os.unlink(temp_path)
   - Return True if all steps succeed, False on any exception

7. validate_quantization(
     original: nn.Module,
     quantized: nn.Module,
     on_failure: str = "error"
   ) -> ValidationResult:
   - Initialize result = ValidationResult()
   - Calculate SQNR via _calculate_model_sqnr, store in result.sqnr_db
   - Calculate sizes via calculate_model_size, store original/quantized sizes and ratio
   - Run load test via _test_load_run, store in result.load_test_passed
   - Run weight range check via _check_weight_ranges, store in result.weight_range_valid
   - If not load_test_passed or not weight_range_valid:
     - Create error message
     - result.errors.append(msg)
     - If on_failure == "error": raise ValueError(msg)
     - Elif on_failure == "warn": import warnings; warnings.warn(msg)
   - Return result

Add module docstring explaining validation checks and on_failure modes.
</action>
  <verify>
from mono_quant.io.validation import calculate_model_size, calculate_sqnr, ValidationResult
import torch.nn as nn

model = nn.Linear(10, 20)
size_mb, params = calculate_model_size(model)
assert size_mb > 0
assert params == 10 * 20 + 20  # weights + bias

w = torch.randn(64, 128)
w_q = torch.quantize_per_channel(w, torch.ones(128)*0.01, torch.zeros(128).int(), 0, torch.qint8)
sqnr = calculate_sqnr(w, w_q)
assert sqnr > 0  # Should have positive SQNR
</verify>
  <done>
Validation metrics module provides calculate_model_size, calculate_sqnr, _check_weight_ranges, _test_load_run, and validate_quantization with configurable on_failure behavior.
</done>
</task>

<task type="auto">
  <name>Task 2: Integrate validation with static_quantize</name>
  <files>src/mono_quant/core/quantizers.py</files>
  <action>
Update src/mono_quant/core/quantizers.py:

1. Import ValidationResult at module level (or use string annotation)
2. Update static_quantize signature:
   - Add on_failure: str = "error" parameter
   - Add run_validation: bool = True parameter

3. After quantization completes in static_quantize:
   - If run_validation:
     - Local import: validate_quantization, calculate_model_size from io.validation
     - Call validate_quantization(original_model, quantized_model, on_failure=on_failure)
     - Extract metrics from ValidationResult
     - Update QuantizationInfo with validation results:
       - sqnr_db, original_size_mb, quantized_size_mb, compression_ratio
   - Return (quantized_model, quantization_info) with validation data

4. Update QuantizationInfo dataclass:
   - Add optional fields: sqnr_db, original_size_mb, quantized_size_mb, compression_ratio

Add docstring noting validation runs by default with on_failure="error" behavior.
</action>
  <verify>
from mono_quant.core.quantizers import static_quantize
import torch.nn as nn

model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
calibration_data = [torch.randn(4, 10) for _ in range(50)]
q_model, info = static_quantize(model, calibration_data, on_failure="warn")
assert hasattr(info, "sqnr_db")
assert hasattr(info, "compression_ratio")
assert info.compression_ratio > 1.0  # Should have compression
</verify>
  <done>
static_quantize integrates automatic validation. QuantizationInfo includes SQNR, size metrics, compression ratio. Validation behavior configurable via on_failure parameter.
</done>
</task>

<task type="auto">
  <name>Task 3: Export public API from package root</name>
  <files>src/mono_quant/__init__.py</files>
  <action>
Update src/mono_quant/__init__.py to export complete public API:

1. From .config: QuantizationConfig
2. From .core.quantizers: dynamic_quantize, static_quantize
3. From .io: save_model, load_model
4. From .io.validation: ValidationResult, validate_quantization (for manual validation)

Update package docstring to describe available functions:
- dynamic_quantize: Dynamic quantization without calibration
- static_quantize: Static quantization with calibration data
- save_model/load_model: Save/load quantized models
- QuantizationConfig: Configuration dataclass
- ValidationResult: Validation metrics dataclass

Add __all__ list for explicit public API:
__all__ = [
    "dynamic_quantize",
    "static_quantize",
    "save_model",
    "load_model",
    "QuantizationConfig",
    "ValidationResult",
    "validate_quantization",
]
</action>
  <verify>
from mono_quant import static_quantize, dynamic_quantize, save_model, load_model
from mono_quant import QuantizationConfig, ValidationResult
assert callable(static_quantize)
assert callable(dynamic_quantize)
assert callable(save_model)
assert callable(load_model)
</verify>
  <done>
Package root exports complete public API: static_quantize, dynamic_quantize, save_model, load_model, QuantizationConfig, ValidationResult.
</done>
</task>

<task type="auto">
  <name>Task 4: Update io module exports</name>
  <files>src/mono_quant/io/__init__.py</files>
  <action>
Update src/mono_quant/io/__init__.py to export:

From .handlers:
- _detect_input_format (for internal use if needed)
- _prepare_model (for internal use)
- _validate_model (for internal use)

From .formats:
- save_model, load_model
- save_pytorch, load_pytorch
- save_safetensors, load_safetensors

From .validation:
- ValidationResult
- calculate_model_size
- calculate_sqnr
- validate_quantization

Add module docstring explaining I/O capabilities.
</action>
  <verify>
from mono_quant.io import save_model, load_model, ValidationResult, validate_quantization
assert callable(save_model)
assert callable(load_model)
assert callable(validate_quantization)
</verify>
  <done>
I/O module exports save/load functions and validation metrics for advanced usage.
</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. from mono_quant import static_quantize - imports successfully (public API)
2. static_quantize returns QuantizationInfo with sqnr_db, compression_ratio
3. calculate_model_size returns (size_mb, param_count) tuple
4. calculate_sqnr returns positive dB value for quantized tensors
5. validate_quantization runs all four checks (SQNR, size, load test, weight range)
6. on_failure="error" raises exception on validation failure
7. on_failure="warn" issues warning but returns model
8. on_failure="ignore" silent on validation failure
9. Quantized model can be saved and loaded via save_model/load_model
</verification>

<success_criteria>
- Validation metrics (SQNR, model size, compression ratio) calculated after quantization
- Load test verifies quantized model can be saved/loaded and run inference
- Weight range check catches numerical issues
- on_failure parameter controls validation failure behavior
- static_quantize exported from public API
- All Phase 2 success criteria from ROADMAP.md met
</success_criteria>

<output>
After completion, create `.planning/phases/02-static-quantization-&-i/02-04-SUMMARY.md`
</output>
